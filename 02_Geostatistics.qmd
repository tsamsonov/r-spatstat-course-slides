---
title: "Геостатистика"
subtitle: "Пространственная статистика"
date: today
date-format: long
author: "Самсонов Тимофей Евгеньевич"
execute:
  echo: false
  freeze: true
engine: knitr
format:
  revealjs: 
    theme: [default, custom.scss]
    margin: 0.2
    width: 1280
    height: 720
    slide-number: true
    footer: "Самсонов Т. Е. Пространственная статистика: курс лекций"
    header-includes: <link rel="stylesheet" media="screen" href="https://fontlibrary.org//face/pt-sans" type="text/css"/>
bibliography: references.yaml
mainfont: PT Sans
---

## Базовые компоненты

1.  Пространственные локации (точки) $$\{p_1, p_2, ..., p_n\}$$
2.  Данные в этих локациях $$\{Z(p_1), Z(p_2), ..., Z(p_n)\}$$ 

> Обе компоненты в общем случае являются случайными

## Случайная величина

> **Случайной величиной** $Z(w)$ называется функция, которая в результате случайного события $w$ принимает некоторое вещественнозначное значение.

::: columns
::: {.column width="50%"}
Например, при анализе температуры водоема в отдельно взятой точке в толще воды случайной величиной (функцией) является собственно температура, а событием — та совокупность физико-химических условий, которая сложилась в данной точке в момент измерений.
:::

::: {.column width="50%"}
*Элемент случайности вносится именно событием*, которое в природе может быть чрезвычайно сложной и трудно предсказуемой комбинацией факторов, в то время как *случайная величина связана с событием функциональной зависимостью*.
:::
:::

## Пространственная модель

$p \in \mathbb{R}^k$ — точка в $k$-мерном Евклидовом пространстве

$Z(p)$ — **случайная величина** в точке $p$

Если $p$ меняется над индексным множеством $D \subset \mathbb{R}^k$, то формируется **случайный процесс**:

$$
\{Z(p) | p \in D\}
$$ 

Результат наблюдения случайного процесса в точках $D$ является **реализацией** случайного процесса:

$$
\{z(p) | p \in D\}
$$ 

> В общем случае $D$ и $Z$ случайны и независимы

## Случайный процесс (функция)

> **Случайный процесс** — это семейство случайных величин, индексированных некоторым параметром $t$

-   Наиболее часто анализируются одномерные случайные процессы, в которых $t$ — это время

-   Пример случайного процесса — температура не в один момент времени, а в течение некоторого промежутка времени

-   Пространственная статистика изучает случайные процессы, в которых $t$ — это координата точки (обычно на плоскости)

## Случайный процесс (функция)

-   В каждой точке $p_i$ существует некоторая *случайная величина* $Z(p_i)$ — **сечение случайного процесса**

-   При изменении точки $p_i$ наблюдаемое значение случайного процесса меняется случайным образом, поскольку определяется оно не только местоположением, но и заранее неизвестным случайным событием

-   Большинство природных явлений показывают зависимость наблюдаемых значений от их взаимного местоположения, которая проявляется в наличии корреляции значений $Z(\mathbf{p})$ и $Z(\mathbf{p} + \mathbf{h})$, где $\mathbf{h}$ — вектор смещения между точками

-   Тем короче $h$, тем, как правило, сильнее выражена корреляция значений

## Автокорреляция

::: columns
::: {.column width="40%"}
![](images/Student.jpg){width="100%"}
:::

::: {.column width="60%"}
Стьюдент в письме Карлу Пирсону (1900):

> *...В целом, корреляция ослабевает, если охват по времени или пространству увеличивается. Меня не покидает мысль, что было бы великим достижением установить закон, согласно которому корреляция будет ослабевать с увеличением охвата*
:::
:::

## Автокорреляция

Британский статистик и биолог **Рональд Эйлмер Фишер** изучал пространственное распределение характеристик растений на опытных площадках, будучи сотрудником Ротамстедской агрохимической станции в 1920-1930-е гг.

::: columns
::: {.column width="40%"}
![Sir Ronald Aylmer Fisher (1876-1937)](images/Fisher.png){width="70%"}
:::

::: {.column width="60%"}
Стьюдент в письме Карлу Пирсону (1900):

> ...всестороннее подтверждение получил тот факт, что близко расположенные участки более схожи, чем удаленные, судя по данным об урожайности... следовательно, наиболее верным будет делать \[выборочные\] блоки максимально компактными\_
>
> `r tufte::quote_footer('--- «The Design of Experiments» (1935)')`
:::
:::

## Пространственная статистика

1.  **Геостатистика** *(geostatistics)*
    -   $D$ — фиксированное подмножество в $\mathbb{R}^k$
    -   $Z(p)$ — случайный вектор в каждой точке $p$
    -   Исследуется пространственное распределение

2.  **Сеточные данные** *(lattice data)*

    -   $D$ — фиксированное счетное подмножество точек $\mathbb{R}^k$
    -   $Z(p)$ — случайный вектор в каждой точке $p$
    -   Исследуется пространственная зависимость и гетерогенность

3.  **Конфигурации точек** *(point patterns)*

    -   $D$ — счетное подмножество точек $\mathbb{R}^k$ *(точечный процесс)*
    -   $Z(p)$ — константа или счетное множество *(маркированный точечный процесс)*
    -   Исследуется пространственное размещение

## Математическое ожидание

**Математическое ожидание** — наиболее вероятная реализация случайного процесса:

$$
\operatorname E[Z(p)]=m(p)
$$

-   Пусть дан географический регион, в котором производятся наблюдения температуры в течение месяца.

-   В каждый момент времени мы имеем непрерывное поле температуры — реализацию случайного процесса и относящиеся к ней данные наблюдений на метеостанциях

-   Осреднив данные за период наблюдений восстановим выборочное среднее поле распределения температур — оценку математического ожидания случайного процесса

## Дисперсия

**Дисперсия** — мера разброса реализаций случайного процесса относительно его математического ожидания:

$$
\operatorname {Var}[Z(p)]= \operatorname E[Z^2(p)]-m^2(p)
$$

> Аналогично математическому ожиданию, дисперсия двумерного с.п. представляет собой *поле распределения*, значение которого в каждой точке равно дисперсии локального сечения с.п.

## Ковариация

**Ковариация** — мера линейной зависимости сечений случайного процесса в двух точках $p_1$ и $p_2$:

$$
\operatorname {Cov}(p_1,p_2) = \operatorname {Cov}[Z(p_1), Z(p_2)] = \\\operatorname E[Z(p_1)Z(p_2)]-m(p_1)m(p_2)
$$

> Недостатком ковариации является необходимость знания математического ожидания с.п. Это условие выполняется далеко не всегда, что связано с тем что как правило приходится иметь дело только с одной реализацией с.п

## Свойства моментов случайных процессов

-   Моменты пространственных случайных процессов являются **функциями**, а не *константами*, в отличие от моментов случайных величин.

-   Давать оценку пространственной структуре явления на основе вычисленных моментов с.п. можно только при условии, что он удовлетворяет свойствам **стационарности** и **эргодичности.**

## Гипотеза стационарности

-   Функции корреляции между данными зависят только от взаимного расположения точек измерений, а не от их конкретного местоположения в пространстве.

-   В этом случае пространственная корреляция определяется вектором $\mathbf{h}$ между точками.

-   Для изотропного случая, когда корреляция не зависит от направления, а только от расстояния вектор $\mathbf{h}$ переходит в скаляр — расстояние $d$.

## Стационарность

**Стационарность** *в строгом смысле* означает что функция распределения множества случайных величин для любой комбинации точек ${x_1, x_2,...,x_k}$ и любого $k < \infty$ остается неизменной при смещении этой комбинации на произвольный вектор $h$:

$$
\operatorname P\{Z(x_1)<z_1,...,Z(x_k)<z_k\} = \\ \operatorname P\{Z(x_1 + h)<z_1,...,Z(x_k + h)<z_k\}
$$

## Стационарность

-   Стационарность по другому называют **однородностью в пространстве**, подразумевая что явление ведет себя одинаковым образом в любой точке пространства, как бы повторяет само себя.

-   Если с.ф. стационарна, все ее моменты будут инвариантны относительно сдвигов (то есть будут постоянны), а это означает что для их оценки можно использовать ограниченную в пространстве область.

-   В реальности же подобного рода «идеальное» поведение встречается крайне редко, поэтому используют более слабое предположение о стационарности второго порядка.

## Стационарность второго порядка

Случайная функция имеет имеет **стационарность второго порядка**, если для любых точек $x$ и $x+h$ в $R^k$

$$
\begin{cases}
  \operatorname E[Z(x)] = m \\
  \operatorname E[(Z(x)-m)(Z(x+h)-m)] = \operatorname C(h)
\end{cases}
$$

Математическое ожидание с.ф. постоянно, а ковариация зависит только от вектора $h$ между точками и не зависит от их абсолютного положения.

Если ковариация также не зависит от направления, а только от расстояния между точками, то $h$ вырождается в скаляр, а такая случайная функция является *изотропной стационарной*.

## Эргодичность

Стационарная случайная функция $Z(x,w)$ называется **эргодической**, если ее среднее по области $V \subset R^k$ сходится к математическому ожиданию $m(w)$ при стремлении $V$ к бесконечности:

$$
\lim_{V \rightarrow \infty} \frac{1}{|V|}\int_{V} Z(x,w)dx = m(w)
$$

$|V|$ обозначает *меру* области $V$ (площадь, объем). Предполагается что сама область $V$ растет во всех направлениях, и предел ее роста не зависит от ее формы.

> Cреднее по всем возможным реализациям равно среднему отдельной безграничной в пространстве реализации.

## Эргодичность

-   Дан кувшин с песком, в котором необходимо определить долю объема, занятую содержимым.

-   Зафиксируем некоторую точку $x$ в системе отсчета, привязанной к кувшину, и будем его встряхивать бесконечное число раз, каждый раз фиксируя, оказалась ли точка $x$ внутри песчинки (записываем 1) или же попала в свободное между ними пространство (записываем 0)

-   Из серии подобных экспериментов мы сможем оценить среднее значение индикаторной функции $\operatorname I(x,w)$, которое равно вероятности попадания зерна в точку $x$, и которое не зависит от $x$.

-   Эта вероятность и будет равна доли объема кувшина, занятой песком.

## Эргодичность

-   Аналогичный результат можно получить, если теперь зафиксировать кувшин, а точку $x$ выбирать каждый раз случайным образом.

-   В первом случае берется среднее по реализациям, а во втором — среднее по пространству.

> В реальных экспериментах приходится иметь дело со вторым случаем.

## Простой кригинг

Для оценки в точке $z_0 = z(p_0)$ по $N$ измерениям $z_1, ..., z_N$ ищутся коэффициенты следующего выражения:

$$
Z^* = \sum_{i} \lambda_i Z_i + \lambda_0
$$

Константа $\lambda_0$ и веса $\lambda_i$ подобираются таким образом, что минимизируется среднеквадратическая ошибка:

$$
\operatorname E\big[(Z^* - Z_0)^2\big],
$$ то есть математическое ожидание квадрата отклонения оценки от реального значения в точке $p_0$.

## Простой кригинг

$$
Z^* = \sum_{i} \lambda_i Z_i + \lambda_0
$$

Используя соотношение $\operatorname {Var}[X] = \operatorname E[X^2] - (\operatorname E[X])^2$, можно выразить среднюю квадратическую ошибку как:

$$
\operatorname E\big[(Z^* - Z_0)^2\big] = \operatorname{Var}[Z^* - Z_0] + (\operatorname E[Z^* - Z_0])^2
$$

Поскольку дисперсия нечувствительна к сдвигам, изменение константы $\lambda_0$ влияет только на компоненту $\operatorname E[Z^* - Z_0]$. Приравняем ее нулю:

$$
\operatorname E[Z^* - Z_0] = \operatorname E\Big[\sum_{i} \lambda_i Z_i + \lambda_0 - Z_0\Big] = 0
$$

## Простой кригинг

$$
\operatorname E[Z^* - Z_0] = \operatorname E\Big[\sum_{i} \lambda_i Z_i + \lambda_0 - Z_0\Big] = 0
$$

Поскольку $\lambda_0$ явялется константой, то по свойству мат. ожидания ее можно вынести за скобки:

$$
\lambda_0 = -\operatorname E\Big[\sum_{i} \lambda_i Z_i - Z_0\Big] = m_0 - \sum_i \lambda_i m_i,
$$

где $m_i$ — известные значения мат. ожиданий случайной функции в каждой точке исходных данных, $m_0$ — известное мат. ожидание в интерполируемой точке.

## Простой кригинг

Имея:

$$
Z^* = \sum_{i} \lambda_i Z_i + \lambda_0,\\
\lambda_0 = m_0 - \sum_i \lambda_i m_i,
$$

Получаем:

$$
Z^* = \sum_{i} \lambda_i Z_i + m_0 - \sum_i \lambda_i m_i = \\
m_0 + \sum_{i} \lambda_i (Z_i - m_i)
$$

## Простой кригинг

$$
Z^* = m_0 + \sum_{i} \lambda_i (Z_i - m_i)
$$

Поскольку константа $m_0$ известна заранее, задачу оценки можно выполнить для переменной $Y(p) = Z(p) - m(p)$, используя линейную оценку

$$
Y^* = \sum_{i} \lambda_i Y_i,
$$

и прибавляя к полученному результату $m_0$.

Основной вопрос заключается в нахождении коэффициентов $\lambda_i$.

## Простой кригинг

Поскольку мы показали, что компонента $\operatorname E[Z^* - Z_0]$ может быть приравнена нулю, среднеквадратическая ошибка равна дисперсии:

$$
\operatorname E\big[(Z^* - Z_0)^2\big] = \operatorname{Var}[Z^* - Z_0]
$$

Используя свойства:

-   $\operatorname{Var}[X + Y] = \operatorname{Var}[X] + \operatorname{Var}[Y] + 2 \operatorname{Cov}[X, Y]$,\
-   $\operatorname{Var}[-X] = \operatorname{Var}[X]$,
-   $\operatorname{Cov}[X, -Y] = -\operatorname{Cov}[X, Y]$, получаем:

$$
\operatorname{Var}[Z^* - Z_0] = \operatorname{Var}[Z^*] + \operatorname{Var}[Z_0] - 2 \operatorname{Cov}[Z^*, Z_0]
$$

## Простой кригинг

$$
\operatorname{Var}[Z^* - Z_0] = \operatorname{Var}[Z^*] + \operatorname{Var}[Z_0] - 2 \operatorname {Cov}[Z^*, Z_0]
$$

Распишем компоненты этого выражения в терминах ковариации.

Пусть $X_1,\ldots, X_n$ случайные величины, а $Y_1 = \sum\limits_{i=1}^n a_i X_i,\; Y_2 = \sum\limits_{j=1}^m b_j X_j$ — их две произвольные линейные комбинации. Тогда:

$$
\operatorname {Cov}[Y_1,Y_2] = \sum\limits_{i=1}^n\sum\limits_{j=1}^m a_i b_j \operatorname {Cov}[X_i,X_j]$$.

## Простой кригинг

$$
\operatorname{Var}[Z^* - Z_0] = \operatorname{Var}[Z^*] + \operatorname{Var}[Z_0] - 2 \operatorname{Cov}[Z^*, Z_0]
$$

Распишем компоненты этого выражения в терминах ковариации.

$\operatorname{Var}[Z^*] = \operatorname{Cov}[Z^*, Z^*] = \operatorname{Cov}\Big[\sum_{i} \lambda_i Z_i, \sum_{j} \lambda_j Z_j\Big] = \\ \sum_{i}\sum_{j} \lambda_i \lambda_j \operatorname{Cov}[Z_i, Z_j] = \sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij}$

$\operatorname{Var}[Z_0] = \operatorname{Cov}[Z_0, Z_0] = \sigma_{00}$

$\operatorname{Cov}[Z^*, Z_0] = \operatorname{Cov}\Big[\sum_{i} \lambda_i Z_i, Z_0\Big] =\\ \sum_{i} \lambda_i \operatorname{Cov}[Z_i, Z_0] = \sum_{i} \lambda_i \sigma_{i0}$

## Простой кригинг

Таким образом, выражение для ошибки

$$
\operatorname{Var}[Z^* - Z_0] = \operatorname{Var}[Z^*] + \operatorname{Var}[Z_0] - 2 \operatorname{Cov}[Z^*, Z_0]
$$

Трансформируется в

$$
\operatorname{Var}[Z^* - Z_0] = \sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00}
$$

Для нахождения минимума этой квадратичной функции необходимо приравнять нулю ее производные по основной переменной $\lambda$. Выберем в качестве «жертвы» коэффициенты с индексом $i$:

$$
\frac{\partial}{\partial \lambda_i} \operatorname E\big[(Z^* - Z_0)^2\big] = 2 \sum_{j} \lambda_j \sigma_{ij} - 2 \sigma_{i0} = 0
$$

## Простой кригинг

$$
\frac{\partial}{\partial \lambda_i} \operatorname E\big[(Z^* - Z_0)^2\big] = 2 \sum_{j} \lambda_j \sigma_{ij} - 2 \sigma_{i0} = 0
$$

Таким образом, система уравнений **простого кригинга** для точки $Z_0$ имеет вид:

$$
\color{red}{\boxed{\color{blue}{\sum_{j} \lambda_j \sigma_{ij} = \sigma_{i0}\color{gray}{,~i = 1,...,N}}}}
$$

> Уравнения простого кригинга носят чисто теоретический характер. На практике используется метод обычного кригинга, в котором знание среднего значения случайной функции не требуется.

## Дисперсия простого кригинга

Существует возможность оценить в каждой точке не только величину показателя, но также дисперсию оценки (в случае постоянного мат. ожидания — среднеквадратическую ошибку).

Для этого необходимо коэффициенты $\lambda_i$, полученные из системы уравнения простого кригинга

$$
\sum_{j} \lambda_j \sigma_{ij} = \sigma_{i0}
$$

подставить в выражение среднеквадратической ошибки

$$
\operatorname{Var}[Z^* - Z_0] = \sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00}
$$

## Дисперсия простого кригинга

Умножим обе части каждого уравнения простого кригинга на $\lambda_i$ и просуммируем все уравнения по $i$:

$$
\sum_{j} \lambda_j \sigma_{ij} = \sigma_{i0}~\Bigg|\times \lambda_i\\
\sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij} = \sum_{i}\lambda_i\sigma_{i0}
$$

Заметим, что левая часть уравнения присутствует в выражении среднеквадратической ошибки:

$$
\operatorname{Var}[Z^* - Z_0] = \color{red}{\sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij}} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00}
$$

## Дисперсия простого кригинга

Заменим $\sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij}$ на $\sum_{i}\lambda_i\sigma_{i0}$ в выражении для ско:

$$
\operatorname{Var}[Z^* - Z_0] = \color{red}{\sum_{i}\sum_{j} \lambda_i \lambda_j \sigma_{ij}} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00} =\\
\sum_{i}\lambda_i\sigma_{i0} - 2 \sum_{} \lambda_i \sigma_{i0} + \sigma_{00}
$$

Отсюда получаем выражение для дисперсии (ошибки) простого кригинга:

$$
\color{red}{\boxed{\color{blue}{\sigma_{SK} = \operatorname{Var}[Z^* - Z_0] = \sigma_{00} - \sum_{i}\lambda_i\sigma_{i0}}}}
$$

## Библиография
