---
title: "Пространственная регшрессия"
subtitle: "Пространственная статистика"
date: today
date-format: long
author: "Самсонов Тимофей Евгеньевич"
execute:
  echo: false
  freeze: true
engine: knitr
format:
  revealjs: 
    theme: [default, custom.scss]
    margin: 0.2
    width: 1280
    height: 720
    slide-number: true
    footer: "Самсонов Т. Е. Пространственная статистика: курс лекций"
    header-includes: <link rel="stylesheet" media="screen" href="https://fontlibrary.org//face/pt-sans" type="text/css"/>
bibliography: references.yaml
mainfont: PT Sans
---

## Базовые компоненты

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, collapse = TRUE, echo = FALSE, fig.align="left")
library(tidyverse)
library(spdep)
library(spatialreg)
library(raster)
library(gstat)
library(stars)
library(tmap)
library(sf)
library(sp)
```

## Особенности случайных процессов в пространстве

-   **Пространственная зависимость** *(spatial dependence)* — наличие автокорреляции наблюдений. Выражается в невыполнении условия независимости остатков линейной регрессии. Устраняется посредством *пространственной регрессии (spatial regression)*.

-   **Пространственная гетерогенность** *(spatial heterogeneity)* — нестационарность процессов, порождающих наблюдаемую переменную. Выражается в неэффективности постоянных коэффициентов линейной регрессии. Устраняется постредством *географически взвешенной регрессии (geographically weighted regression)*.

## Линейная регрессия

Пусть дан вектор $\mathbf{y} = \{y_1, y_2, ... y_n\}$ измерений зависимой переменной, а также матрица $\mathbf{X} = \{x_{ij}\}$ размером $n \times m$, состоящая из значений $m$ независимых переменных для $n$ измерений. В этом случае модель линейной регрессии может быть записана как

$$\mathbf{y} = \mathbf{X} \boldsymbol\beta + \boldsymbol\epsilon,$$

где:

-   $\boldsymbol\beta$ — вектор коэффициентов регрессии;

-   $\boldsymbol\epsilon$ — вектор случайных ошибок, независимо распределенных относительно среднего значения в нуле.

<!-- ## Многомерное нормальное распределение -->

<!-- Многомерное нормальное распределение (МНР) $k$-мерного случайного вектора $\mathbf{X} = (X_1, ..., X_k)^T$ обозначается как: -->

<!-- $$\mathbf{X}\ \sim \mathcal{N}_k(\boldsymbol\mu,\, \boldsymbol\Sigma)$$ -->

<!-- МНР определяется двумя параметрами: -->

<!-- - __математическое ожидание__ ( $k$-мерный вектор): -->

<!-- $$\boldsymbol\mu = \operatorname{E}[\mathbf{X}] = [ \operatorname{E}[X_1], \operatorname{E}[X_2], \ldots, \operatorname{E}[X_k]]^{\rm T}$$ -->

<!-- - __ковариационная матрица__ (размером $k \times k$): -->

<!-- $$\boldsymbol\Sigma = \operatorname{E} [(\mathbf{X} - \boldsymbol\mu)( \mathbf{X} - \boldsymbol \mu)^{\rm T}] =  [ \operatorname{Cov}[X_i, X_j]; 1 \le i,j \le k ]$$ -->

<!-- ## Стандартный нормальный случайный вектор -->

<!-- Вещественнозначный случайный вектор $\mathbf{X} = (X_1, ..., X_k)^T$ называется __стандартным нормальным случайным вектором__, если все его компоненты $X_n$ независимы друг от друга и подчиняются стандартному случаю нормального закона распределения с нулевым математическим ожиданием и единичной дисперсией для всех $n$: -->

<!-- $$X_n \sim \mathcal{N}(0, 1)$$ -->

<!-- В модели линейной регрессии: -->

<!-- $$\boldsymbol\epsilon \sim \mathcal{N}_k(0, \sigma^2 \mathbf{I}),$$ -->

<!-- где $I$ — единичная матрица размером $k \times k$. -->

<!-- ## Расширение класса регрессионных моделей -->

<!-- $$\boldsymbol\epsilon \sim \mathcal{N}_k(0, \sigma^2 \mathbf{I})$$ -->

<!-- Однако если данные получены измерениями по пространству, остатки регрессии могут демонстрировать пространственную ассоциацию (зависимость), как правило свидетельствующую о наличии дополнительных неучтённых факторов. Это означает, что обычная модель регрессии недостаточно хорошо объясняет зависимость. -->

<!-- Чтобы моделировать зависимость остатков, необходим более широкий класс моделей: -->

<!-- $$\boldsymbol\epsilon \sim \mathcal{N}_k(0, \mathbf{C}),$$ -->

<!-- где $\mathbf{C}$ — любая допустимая ковариационная матрица. -->

## Пример

Процент домохозяйств, находящихся во владении

![](images/tyne_ownerocc.png){width="65%"}

## Пример

Уровень безработицы

![](images/tyne_unempl.png){width="65%"}

## Пример

Обычная линейная регрессия

![](images/tyne_regr.png){width="75%"}

## Пример

Остатки регрессии

![](images/tyne_resid.png){width="65%"}

<!-- ## Расширение класса регрессионных моделей -->

<!-- $$\boldsymbol\epsilon \sim \mathcal{N}_k(0, \mathbf{C})$$ -->

<!-- Данная модель решает проблему независимости остатков, однако порождает две других проблемы: -->

<!-- - Если зависимость остатков имеет пространственный характер (ассоциированы остатки в территориально близких локациях), то матрица $\mathbf{C}$ характер этой зависимости не отражает в явном виде. -->

<!-- - Вектор коэффициентов регрессии $\boldsymbol\beta$ может быть получен путем минимизации $\mathbf{y} - \mathbf{X}\boldsymbol\beta$ путем решения $\beta = \big(\mathbf{X}^T \mathbf{CX} \big)^{-1} \mathbf{X}^T \mathbf{X y}$. Однако это требует знания ковариационной матрицы, которая обычно неизвестна. Поэтому как $\mathbf{C}$, так и $\boldsymbol\beta$ калибруются по выборке. -->

## Пространственная регрессия

Для того чтобы учесть пространственную автокорреляцию остатков, в модель линейной регрессии добавляется компонента **пространственной авторегрессии** *(spatial autoregression)*, которая моделирует *пространстенный лаг*:

$$\mathbf{y} = \underbrace{\mathbf{X} \mathbf{\beta}}_{тренд} + \underbrace{\color{red}{\rho\mathbf{Wy}}}_{сигнал} +  \underbrace{\mathbf{\epsilon}}_{шум},$$

-   $\rho$ — коэффициент регрессии, отражающий степень пространственной автокорреляции

-   $\mathbf{W}$ — матрица пространственных весов

> Полученная модель называется моделью **пространственной регрессии** (*spatial regression*).

Компоненты модели (тренд, сигнал и шум) называются **предикторами**.

## Пространственная регрессия

Для получения коэффициентов $\boldsymbol\beta$ и $\rho$ выполним ряд преобразований:

$$\mathbf{y} = \mathbf{X} \mathbf{\beta} + \rho\mathbf{Wy} +  \mathbf{\epsilon}\\
\mathbf{y} - \rho\mathbf{Wy} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}\\
(\mathbf{I} - \rho\mathbf{W})\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$$

Предполагая, что матрица $(\mathbf{I} - \rho\mathbf{W})$ инвертируема, получаем систему уравнений пространственной регрессии:

$$\color{red}{\boxed{\color{blue}{\mathbf{y} = (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{X}\mathbf{\beta} + (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{\epsilon}}}}$$

Данная модель идентична обычной регрессии $\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$, но в ней независимые переменные и ошибки линейно трансформированы умножением на $(\mathbf{I} - \rho\mathbf{W})^{-1}$.

<!-- --- -->

<!-- ## Пространственная регрессия -->

<!-- $$\mathbf{y} = (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{X}\mathbf{\beta} + (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{\epsilon}$$ -->

<!-- Трансформированная ошибка модели будет иметь ковариационную матрицу -->

<!-- $$\mathbf{C} = \sigma^2 \Big[\big(\mathbf{I} - \rho \mathbf{W}\big)^{-1}\Big]^T (\mathbf{I} - \rho\mathbf{W})^{-1}$$ -->

<!-- - Если ковариационная матрица функционально зависит от параметра $\rho$, то она отражает пространственную структуру автокорреляции ошибок. -->

<!-- - Ковариационная матрица должна быть положительно определенной. Для полученного выражения это будет выполняться в случае если $|\rho| \leq 1$ (Griffith, 1988). -->

## Пространственная регрессия

$$\mathbf{y} = \mathbf{X} \mathbf{\beta} + \rho\mathbf{Wy} +  \mathbf{\epsilon}$$

Для нахождения коэффициентов $\boldsymbol\beta$ и $\rho$ используется минимизация квадрата случайной компоненты, которую можно представить как $\mathbf{\epsilon} = \mathbf{y} - \mathbf{X} \mathbf{\beta} - \rho\mathbf{Wy}$:

$$\sum_i \Bigg(y_i - \sum_j \beta_j x_{ij} - \rho \sum_j w_{ij} y_j \Bigg)^2$$

Задача решается в 2 этапа:

-   находится оптимальное значение $\rho$;
-   находится оптимальное значение $\boldsymbol\beta$ путем подстановки в вышеуказанное выражение.

## Пространственная фильтрация

Модель пространственной регрессии может быть использована для осуществления **пространственной фильтрации** — убирания автокорреляционной составляющей.

Для этого необходимо авторегрессионную компоненту (пространственный лаг) перенести в левую часть уравнения:

$$\mathbf{y} = \mathbf{X} \mathbf{\beta} + \rho\mathbf{Wy} + \mathbf{\epsilon}\\
\mathbf{y}^* = \mathbf{y} - \rho\mathbf{Wy} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$$

-   Пространственная фильтрация бывает полезна, когда наблюдается несоответствие масштаба наблюдений и масштаба процесса.

-   Например, статистика по показателю, контролируемому на региональном уровне, собирается по муниципалитетам. В этом случае фильтрация позволяет подобрать параметры $\mathbf{\beta}$, учитывающие наличие высокой пространственной автокорреляци.

## Оценка географического соседства

Примеры иллюстрируются по данным Кировской области:

```{r, fig.width = 5}
options(scipen = 999)

reg_sf = st_read('../r-geo-course/data/Kirov.gpkg', quiet = TRUE)
reg = st_geometry(reg_sf)

ggplot() +
  geom_sf(data = reg, color = "gray50") +
  theme_bw()
```

## Оценка географического соседства

В целом, можно выделить три большие группы методов:

-   Соседи по смежности

-   Соседи по графу

-   Соседи по метрике

## Соседство по смежности

```{r}
knitr::include_graphics('images/QueenRook.png')
```

**Соседство по смежности** основано на топологических отношениях между объектами и применяется при анализе данных, приуроченных к площадным единицам.

## Соседство по смежности

```{r}
nb_queen = poly2nb(reg) # Соседство по правилу ферзя
coords = reg %>% 
  st_centroid() %>% 
  st_coordinates()
par(mar = c(1,1,1,1))
# Теперь рисуем граф:
plot(reg, border = "gray50")
plot(nb_queen, coords, pch = 19, cex = 0.5, add = TRUE)
```

## Соседство по графу

**Соседство по графу** основано на отношениях объектов в [триангуляции Делоне](https://ru.wikipedia.org/wiki/Триангуляция_Делоне).

::: columns
::: {.column width="50%"}
В эту же категорию попадают всевозможные фильтрации триангуляции Делоне, которые удаляют из нее ребра, не удовлетворяющие заданным критериям:

-   сфера влияния

-   граф Гэбриела

-   относительное соседство
:::

::: {.column width="50%"}
```{r, fig.width=5}
nb_tin = tri2nb(coords)
par(mar = c(1,1,1,1))
plot(reg, border = "grey70")
plot(nb_tin, coords, pch = 19, cex = 0.5, add = TRUE)
```
:::
:::

## Соседство по триангуляции Делоне

::: columns
::: {.column width="50%"}
Соседство по триангуляции реализуется путем триангулирования центров территориальных единиц.

-   Внешние ребра идут по выпуклой оболочке множества центров

-   Очень много связей между единицами, которые реально соседними не являются
:::

::: {.column width="50%"}
```{r, fig.width=5}
nb_tin = tri2nb(coords)
par(mar = c(1,1,1,1))
plot(reg, border = "grey70")
plot(nb_tin, coords, pch = 19, cex = 0.5, add = TRUE)
```
:::
:::

## Сфера влияния

```{r, fig.cap=''}
knitr::include_graphics('images/SphereOfInfluence.png')
```

Ребра триангуляции, инцидентные (примыкающие к) данной вершине, сохраняются только если $D \leq 2D_{min}$

## Соседство по сфере влияния

```{r}
nb_tin = soi.graph(nb_tin, coords) %>% graph2nb()
par(mar = c(1,1,1,1))
plot(reg, border = "grey70")
plot(nb_tin, coords, pch = 19, cex = 0.5, add = TRUE)
```

## Граф Гэбриела

```{r, fig.cap=''}
knitr::include_graphics('images/Gabriel.png')
```

В каждом треугольнике ребро сохранятся только тогда, когда построенная на нем окружность не включает третью точку треугольника

## Соседство по графу Гэбриела

```{r}
nb_gab = gabrielneigh(coords) %>% graph2nb()
par(mar = c(1,1,1,1))
plot(reg, border = "grey70")
plot(nb_gab, coords, pch = 19, cex = 0.5, add = TRUE)
```

## Граф относительного соседства

::: columns
::: {.column width="50%"}
Получается путем фильтрации триангуляции Делоне по следующему правилу:

> ребро $A$, соединяющее две вершины $p$ и $q$, будет удалено, если найдется третья вершина $r$, такая что расстояния от нее до $p$ и $q$ ( $B$ и $C$ соответственно) окажутся короче, чем $A$, то есть: $A > B$ **and** $A > C$.
:::

::: {.column width="50%"}
```{r, fig.width=5}
nb_rel = relativeneigh(coords) %>% graph2nb()
par(mar = c(1,1,1,1))
plot(reg, border = "grey70")
plot(nb_rel, coords, pch = 19, cex = 0.5, add = TRUE)
```
:::
:::

## Соседи по метрике

Поиск соседей по метрике — наиболее простой способ определения соседства.

Для его использования необходимо задать метрику (как правило, расстояние между точками), а также критерий фильтрации связей:

-   по количеству ( $k$ ближайших)

-   по расстоянию (не ближе чем $d_1$, но и не далее чем $d_2$).

## Соседи по количеству

```{r}
par(mfrow = c(2,2),
    mar = c(1,1,1,1))
for (i in 1:4){
  nb_knn = knearneigh(coords, k = i) %>% knn2nb()
  
  plot(reg, border = "grey70")
  plot(nb_knn, coords, pch = 19, cex = 0.5, add = TRUE)
  title(main = paste("Ближайшие соседи (k = ", i, ")", sep = ''))
}
```

## Соседи по расстоянию

```{r}
par(mfrow = c(2,2),
    mar = c(1,1,1,1))
for (d in 5:8) {
  dnearnei = dnearneigh(coords, d1 = 0, d2 = 10000 * d)
  
  plot(reg, border = "grey70")
  plot(dnearnei, coords, pch = 19, cex = 0.5, add = TRUE)
  title(main = paste("Ближайшие соседи (d <=", 10000 * d, ")", sep = ''))
}
```

## Пространственные веса

-   Пространственные веса характеризуют силу связи между единицами.

-   Если единицы не являются соседними (по выбранному правилу), то пространственный вес их связи будет равен нулю. Во всех остальных случаях веса будут ненулевыми.

-   Поскольку теоретически каждая единица может быть связана с любой другой единицей, распространена форма представления весов в виде матрицы $W$ размером $N \times N$, где $N$ -- число единиц.

-   На пересечении $i$-й строки и $j$-го столбца матрицы располагается вес связи между $i$-й и $j$-й единицей.

## Бинарная матрица

Если связь есть, то ее вес равен единице (1), если нет — нулю (0)

```{r, fig.width=7}
Wbin = nb2listw(nb_queen, style = "B")
M = listw2mat(Wbin)
par(mar = c(1,1,1,1))

ramp = colorRampPalette(c("white","red"))
levels = 1 / 1:10  # шкала 1, 0.5, 0.33, 0.25 ... 0.1
lattice::levelplot(M,
                   at = levels, 
          col.regions=ramp(10))
```

## Нормированная матрица

Вес $j$-й единицы по отношению к $i$-й равен $1/n_i$, где $n_i$ — количество соседей у $i$.

```{r, fig.width=6}
W = spdep::nb2listw(nb_queen, style = "W")
M = listw2mat(W)

par(mar = c(1,1,1,1))
lattice::levelplot(M, 
          at = levels, 
          col.regions=ramp(10))
```

## Пространственная автокорреляция

```{r, fig.width = 12, fig.height=8}
mun_src = reg_sf

# Чтение таблицы со статистикой
tab = readxl::read_xlsx("../r-geo-course/data/Kirov.xlsx", 1)

# Соединение таблиц
mun = mun_src %>% 
  left_join(tab, by = c("OBJECTID" = "N")) %>% 
  pivot_longer(cols = 22:31,
               names_to = 'month',
               values_to = 'nsick') %>% 
  mutate(month = ordered(month, levels = c('Январь', 'Февраль', 'Март', 
                                           'Апрель', 'Май', 'Июнь', 
                                           'Июль', 'Август', 'Сентябрь', 
                                           'Октябрь', 'Ноябрь', 'Декабрь'))) %>% 
  st_set_geometry('geometry')

# Построение серии карт
ramp = colorRampPalette(c("white", "orange", "red"))
levels = seq(0, 10000, 1000)
nclasses = length(levels) - 1

ggplot() +
  geom_sf(mun, mapping = aes(geometry = geometry, 
                             fill = cut(nsick, levels))) +
  scale_fill_manual(values = ramp(nclasses),
                    labels = paste(levels[-nclasses-1], '-', levels[-1]),
                    guide = guide_legend(reverse = TRUE),
                    drop = FALSE) +
  facet_wrap(~month)
```

## Индекс Морана (Moran's I)

Анализ пространственной автокорреляции осуществляется, как правило, путем вычисления индекса Морана (Moran's I):

$$I = \frac{n \sum^n_{i=1} \sum^n_{j=i} w_{ij} (y_i - \bar y)(y_j - \bar y)}{ \Big[\sum^n_{i=1} \sum^n_{j=i} w_{ij}\Big] \Big[\sum^n_{i=1} (y_i - \bar y)^2\Big]}$$

где:

-   $n$ — количество единиц,
-   $w_{ij}$ — вес пространственной связи между $i$-й и $j$-й единицей,
-   $y_i$ — значение в $i$-й единице,
-   $\bar y$ — выборочное среднее по всем единицам

## Коэффициент корреляции Пирсона

Обратим внимание на то, что индекс Морана по сути и форме записи похож на линейный коэффициент корреляции Пирсона, в котором перебираются все пары соответствующих друг другу значений из рядов $X = \{x_i\}$ и $Y = \{y_i\}$:

$$r_{xy} = \frac{\sum_{i=1}^{n}(x_i - \bar x)(y_i - \bar y)}{\sqrt{\sum_{i=1}^{n}(x_i - \bar x)^2} \sqrt{\sum_{i=1}^{n}(y_i - \bar y)^2}}$$

## Индекс Морана (Moran's I)

Индекс Морана для нормально распределенных данных лежит в диапазоне от -1 до 1:

-   +1 означает детерминированную прямую зависимость — группировку схожих (низких или высоких) значений.

-   0 означает абсолютно случайное распределение (*CSR — complete spatial randomness*)

-   -1 означает детерминированную обратную зависимость — идеальное перемешивание низких и высоких значений, напоминающее шахматную доску

**Математическое ожидание** индекса Морана для случайных данных равно $E[I] = -1/(n-1)$

## Индекс Морана (Moran's I)

Индекс Морана для данных за февраль:

```{r, echo = TRUE}
# Выбираем данные за февраль
feb = mun |> 
  filter(month == 'Февраль')

# Вычисление индекса (тест) Морана
moran.test(feb$nsick, W)
```

## Перестановочный тест Морана

Значения перемешиваются между территориальными единицами и далее строится гистограмма распределения. Значимость индека Морана оценивается по отклонению реального значения от среднего по случайным симуляциям

```{r, fig.height = 6, fig.width=10}
sim = moran.mc(feb$nsick, listw = W, nsim = 10000)

# Построим гистограмму по вычисленным индексам:
hist(sim$res,
     freq = TRUE,
     breaks = 20, 
     xlim = c(-1,1),
     main = NULL, 
     xlab = "Случайный индекс Морана",
     ylab = "Частота появления",
     col = "steelblue")

# Нанесем фактическое значение
abline(v = sim$statistic, col = "red")
```

## Перестановочный тест Морана

Значения перемешиваются между территориальными единицами и далее строится гистограмма распределения. Значимость индека Морана оценивается по отклонению

```{r}
sim
```

## Диаграмма рассеяния Морана

::: columns
::: {.column width="50%"}
```{r, fig.width=5, fig.height=5}
par(mar = c(1,1,1,1))
moran.plot(feb$nsick, W)
```
:::

::: {.column width="50%"}
По оси $X$ откладывается значение в каждой территориальной единице, в по оси $Y$ — ее пространственный лаг (средневзвешенное значение по всем ее соседям).

-   Тангенс угла наклона прямой равен значению индекса Морана.

-   Выделенные фонариком единицы вносят наибольший вклад в индекс
:::
:::

## Пространственная авторегрессия

Поиск уравнения пространственной регрессии и его авторегрессионной составляющей может быть выполнен посредством функций `spautolm()` и `lagsarlm()` из пакета **spatialreg**:

```{r, echo = TRUE}
(model = lagsarlm(nsick ~ 1, data = feb, listw = W))
```

## Пространственная авторегрессия

На основе полученной модели можно построить карты пространственной авторегрессии и остатков:

```{r, fig.width=10, fig.height=5}
# Извлекаем результаты пространственной авторегрессии
feb_spreg = feb %>% 
  mutate(fitted = fitted(model),
         residual = residuals(model)) %>% 
  pivot_longer(cols = c(nsick, fitted, residual), 
               names_to = 'type',
               values_to = 'value') %>% 
  st_set_geometry('geometry')

# Построение серии карт
ramp = colorRampPalette(c('steelblue3', 'white', 'orange', 'violetred'))
levels = seq(-3000, 10000, 1000)
nclasses = length(levels) - 1

# Сравниваем исходные данные, модельные и остатки
ggplot() +
  geom_sf(feb_spreg, mapping = aes(geometry = geometry, fill = cut(value, levels))) +
  scale_fill_manual(values = ramp(nclasses),
                    labels = paste(levels[-nclasses-1], '-', levels[-1]),
                    guide = guide_legend(reverse = TRUE),
                    drop = FALSE) +
  facet_wrap(~type)
```

## Предсказание на основе простр. регрессии

Различают три вида предсказания:

-   **Внутривыборочное** (in-sample) используется для вычисления предикторов на основе данных, использованных для построения модели пространственной регрессии.

-   **Прогнозное** (prevision/forecast) используется для вычисления предикторов на основе новых данных по тем же выборочным единицам

-   **Вневыборочное** (out-of-sample) используется для вычисления предикторов с включением новых выборочных единиц

## Внутривыборочное предсказание

**Внутривыброчное** (in-sample) предсказание не требует дополнительных действий, поскольку оно осуществляется непосредственно моделью пространственной регрессии:

$$\underbrace{\mathbf{y}}_{отклик} = \underbrace{\mathbf{X} \mathbf{\beta}}_{тренд} + \underbrace{\rho\mathbf{Wy}}_{сигнал} + \underbrace{\mathbf{\epsilon}}_{шум}$$

## Прогнозное предсказание

**Прогонозное** (forecast) предсказание требует последовательного вычисления тренда, переменной отклика и сигнала. Для этого необходимо выполнить следующие преобразования:

$$\mathbf{y} = \mathbf{X} \mathbf{\beta} + \rho\mathbf{Wy} + \mathbf{\epsilon}\\
(\mathbf{I} - \rho\mathbf{W})\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\\
\mathbf{y} = (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{X}\mathbf{\beta} + (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{\epsilon}$$

Прогноз с использованием полученного выражения делается в предположении, что $\mathbf{\epsilon} = 0$. В этом случае, имея новые данные тренда $\mathbf{X}\mathbf{\beta}$, вычисляем сначала $\mathbf{y} = (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{X}\mathbf{\beta}$ и далее находим сигнал их умножением:

$$\rho\mathbf{Wy} = \rho\mathbf{W} (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{X}\mathbf{\beta}$$

## Вневыборочное предсказание

::: columns
::: {.column width="50%"}
![](images/outsample.png){width="100%"}
:::

::: {.column width="50%"}
*Goulard, M., Laurent, T., Thomas-Agnan, C.*, 2017. **About predictions in spatial autoregressive models: optimal and almost optimal strategies**. Spatial Economic Analysis 12, 304–325. https://doi.org/10.1080/17421772.2017.1300679
:::
:::

## Вневыборочное предсказание

**Вневыборочное** (out-of-sample) предсказание связано с решением ситуации, когда есть данные по независимым переменным $\mathbf{X_S}$ и отклику $\mathbf{Y_S}$ по одним единицам и данные только по независимым переменным $\mathbf{X_O}$ для другой части единиц. Требуется найти отклик $\mathbf{Y_O}$ для этих единиц.

Для решения этой задачи необходимо стратифицировать вектора данных и матрицу весов:

$$\begin{bmatrix}\mathbf{Y_S}\\ \color{red}{\mathbf{Y_O}}\end{bmatrix} = \rho \begin{bmatrix}\mathbf{W_{SS}} & \mathbf{W_{SO}} \\ \mathbf{W_{OS}} & \mathbf{W_{OO}}\end{bmatrix} \begin{bmatrix}\mathbf{Y_S}\\\color{red}{\mathbf{Y_O}}\end{bmatrix} + \begin{bmatrix}\mathbf{X_S}\\\mathbf{X_O}\end{bmatrix} \mathbf{\beta} + \begin{bmatrix}\mathbf{\epsilon_S}\\\mathbf{\epsilon_O}\end{bmatrix}$$ Красным цветом выделены неизвестные данные.

## Вневыборочное предсказание

Для вневыборочного предсказания используется ранее полученная модель прогнозирования $\mathbf{y} = (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{X}\mathbf{\beta}$. Однако ее необходимо также представить в стратифицированном виде:

$$\begin{bmatrix}\mathbf{Y_S}\\ \mathbf{Y_O}\end{bmatrix} = (\mathbf{I} - \rho\mathbf{W})^{-1}\mathbf{X}\mathbf{\beta}=\\=\begin{bmatrix}\mathbf{I_{SS}} - \rho\mathbf{W_{SS}} & -\rho\mathbf{W_{SO}} \\ -\rho\mathbf{W_{OS}} & \mathbf{I_{OO}} - \rho\mathbf{W_{OO}}\end{bmatrix}^{-1} \begin{bmatrix}\mathbf{X_S}\\\mathbf{X_O}\end{bmatrix}\beta=\\=\begin{bmatrix}\mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D}\end{bmatrix}^{-1} \begin{bmatrix}\mathbf{X_S}\\\mathbf{X_O}\end{bmatrix}\beta$$

## Вневыборочное предсказание

$$\begin{bmatrix}\mathbf{Y_S}\\ \mathbf{Y_O}\end{bmatrix} = \begin{bmatrix}\mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D}\end{bmatrix}^{-1} \begin{bmatrix}\mathbf{X_S}\\\mathbf{X_O}\end{bmatrix}\beta$$Раскрыв данное выражение, можно получить предсказания для выборочных и вневыборочных единиц:

$$\mathbf{Y_S} = (\mathbf{A} - \mathbf{BD}^{-1} \mathbf{C})^{-1} \mathbf{X_S} \mathbf{\beta} -(\mathbf{A} - \mathbf{BD}^{-1} \mathbf{C})^{-1} \mathbf{BD}^{-1} \mathbf{X_O} \mathbf{\beta}$$

$$\mathbf{Y_O} = (\mathbf{D} - \mathbf{CA}^{-1} \mathbf{B})^{-1} \mathbf{X_O} \mathbf{\beta} - (\mathbf{D} - \mathbf{CA}^{-1} \mathbf{B})^{-1} \mathbf{C} \mathbf{A}^{-1} \mathbf{X_S} \mathbf{\beta}$$

## Исходные данные (пример)

```{r, fig.height=2.5, fig.width=5, dpi=300}
library(sf)
library(tidyverse)
library(spdep)
library(spatialreg)

reg = read_sf(system.file("shapes/columbus.shp", package="spData")[1])

crime_breaks = scales::fullseq(range(reg$CRIME), 10)

ggplot(data = reg) +
  geom_sf(aes(fill = CRIME), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'PuRd', breaks = crime_breaks, direction = 1) + 
  theme_void() + ggtitle("Уровень преступности") +
  theme(plot.title=element_text(hjust=0.5))
```

## Исходные данные (пример)

```{r, fig.height=2.5, fig.width=5, dpi=300}
inc_breaks = scales::fullseq(range(reg$INC), 5)
ggplot(data = reg) +
  geom_sf(aes(fill = INC), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'Greens', breaks = inc_breaks, direction = 1) + 
  theme_void() + ggtitle("Доходы") +
  theme(plot.title=element_text(hjust=0.5))
```

## Исходные данные (пример)

```{r, fig.height=2.5, fig.width=5, dpi=300}
hoval_breaks = scales::fullseq(range(reg$HOVAL), 10)
ggplot(data = reg) +
  geom_sf(aes(fill = HOVAL), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'YlOrBr', breaks = hoval_breaks, direction = 1) + 
  theme_void() + ggtitle("Стоимость домовладения") +
  theme(plot.title=element_text(hjust=0.5))
```

## Диаграмма рассеяния

Диаграмма рассеяния показывает соотношение переменных

```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg, aes(INC, CRIME)) +
  geom_point()
```

## Линейная регрессия

Линейная регрессия дает аппроксимацию зависимости

```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg, aes(INC, CRIME)) +
  geom_point() +
  geom_smooth(method = 'lm')
```

Коэффициент корреляции равен $-0.696$.

## Линейная регрессия

Линейная регрессия дает аппроксимацию зависимости

```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg, aes(HOVAL, CRIME)) +
  geom_point() +
  geom_smooth(method = 'lm')
```

Коэффициент корреляции равен $-0.574$.

## Линейная регрессия

Для модели

$$
\texttt{CRIME} = \beta_0 + \beta_1 \texttt{INC} + \beta_2 \texttt{HOVAL}
$$

получается следующая диагностика:

```{r}
model = lm(CRIME ~ INC + HOVAL, data = reg)
reg = reg |> 
  mutate(
    FIT = fitted(model),
    RES = residuals(model)
  )
s = summary(model)
s$coefficients[,-3]
```

Т.е. модель принимает следующий вид:

$$
\texttt{CRIME} = 68.619 -1.597~\texttt{INC} - 0.274~\texttt{HOVAL}
$$

## Линейная регрессия

```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = CRIME), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'PuRd', breaks = crime_breaks, direction = 1) + 
  theme_void() + ggtitle("Уровень преступности (данные)") +
  theme(plot.title=element_text(hjust=0.5))
```

## Линейная регрессия

```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = FIT), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'PuRd', breaks = crime_breaks, direction = 1) + 
  theme_void() + ggtitle("Уровень преступности (модель)") +
  theme(plot.title=element_text(hjust=0.5))
```

## Линейная регрессия

```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = RES), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'Oranges', breaks = crime_breaks, direction = 1) + 
  theme_void() + ggtitle("Уровень преступности (остатки)") +
  theme(plot.title=element_text(hjust=0.5))
```

## Остатки регрессии

::: columns
::: {.column width="50%"}
::: callout-important
## Важно

Если остатки от регрессии образуют пространственный рисунок, это значит, что независимых переменных недостаточно для предсказания исследуемой величины. Необходимо учитывать пространственную зависимость.
:::
:::

::: {.column width="50%"}
```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = RES), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'Oranges', breaks = crime_breaks, direction = 1) + 
  theme_void() + ggtitle("Уровень преступности (остатки)") +
  theme(plot.title=element_text(hjust=0.5))
```
:::
:::

При анализе карт остатков регрессии обращают внимание на то, меняются ли они плавно по пространству, есть ли выраженный пространственный тренд и зависимость значений соседних единиц.

## Пространственные веса

```{r, fig.height=3, fig.width=4, dpi=300}
nb_queen = poly2nb(reg)
Wbin = nb2listw(nb_queen, style = "B")
M = listw2mat(Wbin)

mdf = data.frame(row = c(row(M)), col = c(col(M)), W = c(M))

ggplot(data = mdf, aes(row, col, fill = W)) +
  scale_fill_distiller(palette = 'Blues', direction = 1) +
  geom_raster() +
  coord_fixed() +
  theme_bw() + ggtitle("Бинарные веса") +
  labs(x = 'Объект', y = 'Объект') +
  theme(plot.title=element_text(hjust=0.5))
```

## Пространственные веса

```{r, fig.height=3, fig.width=4, dpi=300}
Wstand = spdep::nb2listw(nb_queen, style = "W")
M = listw2mat(Wstand)

mdf = data.frame(row = c(row(M)), col = c(col(M)), W = c(M))

ggplot(data = mdf, aes(row, col, fill = W)) +
  scale_fill_distiller(palette = 'Blues', direction = 1) +
  geom_raster() +
  coord_fixed() +
  theme_bw() + ggtitle("Нормированные веса") +
  labs(x = 'Объект', y = 'Объект') +
  theme(plot.title=element_text(hjust=0.5))
```

## Индекс Морана

::: columns
::: {.column width="50%"}
```{r, fig.height=2, fig.width=3, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = CRIME), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'PuRd', breaks = crime_breaks, direction = 1) + 
  theme_void() + ggtitle("Исходные данные") +
  theme(plot.title=element_text(hjust=0.5))
```

Индекс Морана равен $0.500$
:::

::: {.column width="50%"}
```{r, fig.height=2, fig.width=3, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = RES), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'Oranges', breaks = crime_breaks, direction = 1) + 
  theme_void() + ggtitle("Остатки регрессии") +
  theme(plot.title=element_text(hjust=0.5))
```

Индекс Морана равен $0.222$
:::
:::

Поскольку остатки регрессии по-прежнему автокоррелированы, можно сделать вывод о том, что независимые переменные не объясняют полностью величину преступности.

## Пространственная регрессия

Для нашего случая модель будет иметь вид:

$$
\texttt{CRIME} = 45.603 -1.049~\texttt{INC} - 0.266~\texttt{HOVAL} + 0.423~W~\texttt{CRIME}
$$

```{r}
spmodel = lagsarlm(CRIME ~ INC + HOVAL, 
                   data = reg, 
                   listw = Wstand)
spmodel
```

## Пространственная регрессия

```{r, fig.height=3, fig.width=5, dpi=300}
reg = reg |> 
  mutate(SPFIT = fitted(spmodel),
         SPLAG = lag.listw(Wstand, reg$CRIME),
         SPRES = residuals(spmodel),
         SPTREND = CRIME - SPLAG - SPRES,
         FCRIME = CRIME - SPLAG)

ggplot(data = reg) +
  geom_sf(aes(fill = CRIME), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'PuRd', breaks = crime_breaks, direction = 1, limits = c(0,60)) + 
  theme_void() + ggtitle("Исходные данные") +
  theme(plot.title=element_text(hjust=0.5))
```

## Пространственная регрессия

```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = SPFIT), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'PuRd', breaks = crime_breaks, direction = 1, limits = c(0,60)) + 
  theme_void() + ggtitle("Пространственная регрессия") +
  theme(plot.title=element_text(hjust=0.5))
```

## Пространственная регрессия

```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = SPLAG), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'PuRd', breaks = crime_breaks, direction = 1, limits = c(0,60)) + 
  theme_void() + ggtitle("Сигнал (пространственный лаг)") +
  theme(plot.title=element_text(hjust=0.5))
```

## Пространственная регрессия

```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = SPTREND), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'PuBuGn', breaks = crime_breaks, direction = 1) + 
  theme_void() + ggtitle("Тренд") +
  theme(plot.title=element_text(hjust=0.5))
```

## Пространственная регрессия

```{r, fig.height=3, fig.width=5, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = SPRES), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'Oranges', breaks = crime_breaks, direction = 1) + 
  theme_void() + ggtitle("Остатки пространственной регрессии") +
  theme(plot.title=element_text(hjust=0.5))
```

## Остатки пространств. регрессии

**Индекс Морана** для остатков пространств. регрессии равен $0.033$.

::: columns
::: {.column width="40%"}
Автокорреляционная составляющая практически полностью учтена в модели пространственной регрессии. Предсказательная сила модели улучшена.
:::

::: {.column width="60%"}
```{r, fig.height=2, fig.width=3, dpi=300}
ggplot(data = reg) +
  geom_sf(aes(fill = SPRES), linewidth = 0.5, color = 'black') +
  scale_fill_fermenter(palette = 'Oranges', breaks = crime_breaks, direction = 1) + 
  theme_void() + ggtitle("Остатки") +
  theme(plot.title=element_text(hjust=0.5))
```
:::
:::
