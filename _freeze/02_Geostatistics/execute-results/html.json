{
  "hash": "1f4c667f51d28065988e66fd0ea11512",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Геостатистика\"\nsubtitle: \"Пространственная статистика\"\ndate: today\ndate-format: long\nauthor: \"Самсонов Тимофей Евгеньевич\"\nexecute:\n  echo: false\n  freeze: true\nengine: knitr\nformat:\n  revealjs: \n    theme: [default, custom.scss]\n    margin: 0.2\n    width: 1280\n    height: 720\n    slide-number: true\n    footer: \"Самсонов Т. Е. Пространственная статистика: курс лекций\"\n    header-includes: <link rel=\"stylesheet\" media=\"screen\" href=\"https://fontlibrary.org//face/pt-sans\" type=\"text/css\"/>\nbibliography: references.yaml\nmainfont: PT Sans\n---\n\n\n## Базовые компоненты\n\n1.  Пространственные локации (точки) $$\\{p_1, p_2, ..., p_n\\}$$\n2.  Данные в этих локациях $$\\{Z(p_1), Z(p_2), ..., Z(p_n)\\}$$ \n\n> Обе компоненты в общем случае являются случайными\n\n## Случайная величина\n\n> **Случайной величиной** $Z(w)$ называется функция, которая в результате случайного события $w$ принимает некоторое вещественнозначное значение.\n\n::: columns\n::: {.column width=\"50%\"}\nНапример, при анализе температуры водоема в отдельно взятой точке в толще воды случайной величиной (функцией) является собственно температура, а событием — та совокупность физико-химических условий, которая сложилась в данной точке в момент измерений.\n:::\n\n::: {.column width=\"50%\"}\n*Элемент случайности вносится именно событием*, которое в природе может быть чрезвычайно сложной и трудно предсказуемой комбинацией факторов, в то время как *случайная величина связана с событием функциональной зависимостью*.\n:::\n:::\n\n## Пространственная модель\n\n$p \\in \\mathbb{R}^k$ — точка в $k$-мерном Евклидовом пространстве\n\n$Z(p)$ — **случайная величина** в точке $p$\n\nЕсли $p$ меняется над индексным множеством $D \\subset \\mathbb{R}^k$, то формируется **случайный процесс**:\n\n$$\n\\{Z(p) | p \\in D\\}\n$$ \n\nРезультат наблюдения случайного процесса в точках $D$ является **реализацией** случайного процесса:\n\n$$\n\\{z(p) | p \\in D\\}\n$$ \n\n> В общем случае $D$ и $Z$ случайны и независимы\n\n## Случайный процесс (функция)\n\n> **Случайный процесс** — это семейство случайных величин, индексированных некоторым параметром $t$\n\n-   Наиболее часто анализируются одномерные случайные процессы, в которых $t$ — это время\n\n-   Пример случайного процесса — температура не в один момент времени, а в течение некоторого промежутка времени\n\n-   Пространственная статистика изучает случайные процессы, в которых $t$ — это координата точки (обычно на плоскости)\n\n## Случайный процесс (функция)\n\n-   В каждой точке $p_i$ существует некоторая *случайная величина* $Z(p_i)$ — **сечение случайного процесса**\n\n-   При изменении точки $p_i$ наблюдаемое значение случайного процесса меняется случайным образом, поскольку определяется оно не только местоположением, но и заранее неизвестным случайным событием\n\n-   Большинство природных явлений показывают зависимость наблюдаемых значений от их взаимного местоположения, которая проявляется в наличии корреляции значений $Z(\\mathbf{p})$ и $Z(\\mathbf{p} + \\mathbf{h})$, где $\\mathbf{h}$ — вектор смещения между точками\n\n-   Тем короче $h$, тем, как правило, сильнее выражена корреляция значений\n\n## Автокорреляция\n\n::: columns\n::: {.column width=\"40%\"}\n![](images/Student.jpg){width=\"100%\"}\n:::\n\n::: {.column width=\"60%\"}\nСтьюдент в письме Карлу Пирсону (1900):\n\n> *...В целом, корреляция ослабевает, если охват по времени или пространству увеличивается. Меня не покидает мысль, что было бы великим достижением установить закон, согласно которому корреляция будет ослабевать с увеличением охвата*\n:::\n:::\n\n## Автокорреляция\n\nБританский статистик и биолог **Рональд Эйлмер Фишер** изучал пространственное распределение характеристик растений на опытных площадках, будучи сотрудником Ротамстедской агрохимической станции в 1920-1930-е гг.\n\n::: columns\n::: {.column width=\"40%\"}\n![Sir Ronald Aylmer Fisher (1876-1937)](images/Fisher.png){width=\"70%\"}\n:::\n\n::: {.column width=\"60%\"}\nСтьюдент в письме Карлу Пирсону (1900):\n\n> ...всестороннее подтверждение получил тот факт, что близко расположенные участки более схожи, чем удаленные, судя по данным об урожайности... следовательно, наиболее верным будет делать \\[выборочные\\] блоки максимально компактными\\_\n>\n> <footer>--- «The Design of Experiments» (1935)</footer>\n:::\n:::\n\n## Пространственная статистика\n\n1.  **Геостатистика** *(geostatistics)*\n    -   $D$ — фиксированное подмножество в $\\mathbb{R}^k$\n    -   $Z(p)$ — случайный вектор в каждой точке $p$\n    -   Исследуется пространственное распределение\n\n2.  **Сеточные данные** *(lattice data)*\n\n    -   $D$ — фиксированное счетное подмножество точек $\\mathbb{R}^k$\n    -   $Z(p)$ — случайный вектор в каждой точке $p$\n    -   Исследуется пространственная зависимость и гетерогенность\n\n3.  **Конфигурации точек** *(point patterns)*\n\n    -   $D$ — счетное подмножество точек $\\mathbb{R}^k$ *(точечный процесс)*\n    -   $Z(p)$ — константа или счетное множество *(маркированный точечный процесс)*\n    -   Исследуется пространственное размещение\n\n## Математическое ожидание\n\n**Математическое ожидание** — наиболее вероятная реализация случайного процесса:\n\n$$\n\\operatorname E[Z(p)]=m(p)\n$$\n\n-   Пусть дан географический регион, в котором производятся наблюдения температуры в течение месяца.\n\n-   В каждый момент времени мы имеем непрерывное поле температуры — реализацию случайного процесса и относящиеся к ней данные наблюдений на метеостанциях\n\n-   Осреднив данные за период наблюдений восстановим выборочное среднее поле распределения температур — оценку математического ожидания случайного процесса\n\n## Дисперсия\n\n**Дисперсия** — мера разброса реализаций случайного процесса относительно его математического ожидания:\n\n$$\n\\operatorname {Var}[Z(p)]= \\operatorname E[Z^2(p)]-m^2(p)\n$$\n\n> Аналогично математическому ожиданию, дисперсия двумерного с.п. представляет собой *поле распределения*, значение которого в каждой точке равно дисперсии локального сечения с.п.\n\n## Ковариация\n\n**Ковариация** — мера линейной зависимости сечений случайного процесса в двух точках $p_1$ и $p_2$:\n\n$$\n\\operatorname {Cov}(p_1,p_2) = \\operatorname {Cov}[Z(p_1), Z(p_2)] = \\\\\\operatorname E[Z(p_1)Z(p_2)]-m(p_1)m(p_2)\n$$\n\n> Недостатком ковариации является необходимость знания математического ожидания с.п. Это условие выполняется далеко не всегда, что связано с тем что как правило приходится иметь дело только с одной реализацией с.п\n\n## Свойства моментов случайных процессов\n\n-   Моменты пространственных случайных процессов являются **функциями**, а не *константами*, в отличие от моментов случайных величин.\n\n-   Давать оценку пространственной структуре явления на основе вычисленных моментов с.п. можно только при условии, что он удовлетворяет свойствам **стационарности** и **эргодичности.**\n\n## Гипотеза стационарности\n\n-   Функции корреляции между данными зависят только от взаимного расположения точек измерений, а не от их конкретного местоположения в пространстве.\n\n-   В этом случае пространственная корреляция определяется вектором $\\mathbf{h}$ между точками.\n\n-   Для изотропного случая, когда корреляция не зависит от направления, а только от расстояния вектор $\\mathbf{h}$ переходит в скаляр — расстояние $d$.\n\n## Стационарность\n\n**Стационарность** *в строгом смысле* означает что функция распределения множества случайных величин для любой комбинации точек ${x_1, x_2,...,x_k}$ и любого $k < \\infty$ остается неизменной при смещении этой комбинации на произвольный вектор $h$:\n\n$$\n\\operatorname P\\{Z(x_1)<z_1,...,Z(x_k)<z_k\\} = \\\\ \\operatorname P\\{Z(x_1 + h)<z_1,...,Z(x_k + h)<z_k\\}\n$$\n\n## Стационарность\n\n-   Стационарность по другому называют **однородностью в пространстве**, подразумевая что явление ведет себя одинаковым образом в любой точке пространства, как бы повторяет само себя.\n\n-   Если с.ф. стационарна, все ее моменты будут инвариантны относительно сдвигов (то есть будут постоянны), а это означает что для их оценки можно использовать ограниченную в пространстве область.\n\n-   В реальности же подобного рода «идеальное» поведение встречается крайне редко, поэтому используют более слабое предположение о стационарности второго порядка.\n\n## Стационарность второго порядка\n\nСлучайная функция имеет имеет **стационарность второго порядка**, если для любых точек $x$ и $x+h$ в $R^k$\n\n$$\n\\begin{cases}\n  \\operatorname E[Z(x)] = m \\\\\n  \\operatorname E[(Z(x)-m)(Z(x+h)-m)] = \\operatorname C(h)\n\\end{cases}\n$$\n\nМатематическое ожидание с.ф. постоянно, а ковариация зависит только от вектора $h$ между точками и не зависит от их абсолютного положения.\n\nЕсли ковариация также не зависит от направления, а только от расстояния между точками, то $h$ вырождается в скаляр, а такая случайная функция является *изотропной стационарной*.\n\n## Эргодичность\n\nСтационарная случайная функция $Z(x,w)$ называется **эргодической**, если ее среднее по области $V \\subset R^k$ сходится к математическому ожиданию $m(w)$ при стремлении $V$ к бесконечности:\n\n$$\n\\lim_{V \\rightarrow \\infty} \\frac{1}{|V|}\\int_{V} Z(x,w)dx = m(w)\n$$\n\n$|V|$ обозначает *меру* области $V$ (площадь, объем). Предполагается что сама область $V$ растет во всех направлениях, и предел ее роста не зависит от ее формы.\n\n> Cреднее по всем возможным реализациям равно среднему отдельной безграничной в пространстве реализации.\n\n## Эргодичность\n\n-   Дан кувшин с песком, в котором необходимо определить долю объема, занятую содержимым.\n\n-   Зафиксируем некоторую точку $x$ в системе отсчета, привязанной к кувшину, и будем его встряхивать бесконечное число раз, каждый раз фиксируя, оказалась ли точка $x$ внутри песчинки (записываем 1) или же попала в свободное между ними пространство (записываем 0)\n\n-   Из серии подобных экспериментов мы сможем оценить среднее значение индикаторной функции $\\operatorname I(x,w)$, которое равно вероятности попадания зерна в точку $x$, и которое не зависит от $x$.\n\n-   Эта вероятность и будет равна доли объема кувшина, занятой песком.\n\n## Эргодичность\n\n-   Аналогичный результат можно получить, если теперь зафиксировать кувшин, а точку $x$ выбирать каждый раз случайным образом.\n\n-   В первом случае берется среднее по реализациям, а во втором — среднее по пространству.\n\n> В реальных экспериментах приходится иметь дело со вторым случаем.\n\n## Простой кригинг\n\nДля оценки в точке $z_0 = z(p_0)$ по $N$ измерениям $z_1, ..., z_N$ ищутся коэффициенты следующего выражения:\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0\n$$\n\nКонстанта $\\lambda_0$ и веса $\\lambda_i$ подобираются таким образом, что минимизируется среднеквадратическая ошибка:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big],\n$$ то есть математическое ожидание квадрата отклонения оценки от реального значения в точке $p_0$.\n\n## Простой кригинг\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0\n$$\n\nИспользуя соотношение $\\operatorname {Var}[X] = \\operatorname E[X^2] - (\\operatorname E[X])^2$, можно выразить среднюю квадратическую ошибку как:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big] = \\operatorname{Var}[Z^* - Z_0] + (\\operatorname E[Z^* - Z_0])^2\n$$\n\nПоскольку дисперсия нечувствительна к сдвигам, изменение константы $\\lambda_0$ влияет только на компоненту $\\operatorname E[Z^* - Z_0]$. Приравняем ее нулю:\n\n$$\n\\operatorname E[Z^* - Z_0] = \\operatorname E\\Big[\\sum_{i} \\lambda_i Z_i + \\lambda_0 - Z_0\\Big] = 0\n$$\n\n## Простой кригинг\n\n$$\n\\operatorname E[Z^* - Z_0] = \\operatorname E\\Big[\\sum_{i} \\lambda_i Z_i + \\lambda_0 - Z_0\\Big] = 0\n$$\n\nПоскольку $\\lambda_0$ явялется константой, то по свойству мат. ожидания ее можно вынести за скобки:\n\n$$\n\\lambda_0 = -\\operatorname E\\Big[\\sum_{i} \\lambda_i Z_i - Z_0\\Big] = m_0 - \\sum_i \\lambda_i m_i,\n$$\n\nгде $m_i$ — известные значения мат. ожиданий случайной функции в каждой точке исходных данных, $m_0$ — известное мат. ожидание в интерполируемой точке.\n\n## Простой кригинг\n\nИмея:\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0,\\\\\n\\lambda_0 = m_0 - \\sum_i \\lambda_i m_i,\n$$\n\nПолучаем:\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + m_0 - \\sum_i \\lambda_i m_i = \\\\\nm_0 + \\sum_{i} \\lambda_i (Z_i - m_i)\n$$\n\n## Простой кригинг\n\n$$\nZ^* = m_0 + \\sum_{i} \\lambda_i (Z_i - m_i)\n$$\n\nПоскольку константа $m_0$ известна заранее, задачу оценки можно выполнить для переменной $Y(p) = Z(p) - m(p)$, используя линейную оценку\n\n$$\nY^* = \\sum_{i} \\lambda_i Y_i,\n$$\n\nи прибавляя к полученному результату $m_0$.\n\nОсновной вопрос заключается в нахождении коэффициентов $\\lambda_i$.\n\n## Простой кригинг\n\nПоскольку мы показали, что компонента $\\operatorname E[Z^* - Z_0]$ может быть приравнена нулю, среднеквадратическая ошибка равна дисперсии:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big] = \\operatorname{Var}[Z^* - Z_0]\n$$\n\nИспользуя свойства:\n\n-   $\\operatorname{Var}[X + Y] = \\operatorname{Var}[X] + \\operatorname{Var}[Y] + 2 \\operatorname{Cov}[X, Y]$,\\\n-   $\\operatorname{Var}[-X] = \\operatorname{Var}[X]$,\n-   $\\operatorname{Cov}[X, -Y] = -\\operatorname{Cov}[X, Y]$, получаем:\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname{Cov}[Z^*, Z_0]\n$$\n\n## Простой кригинг\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname {Cov}[Z^*, Z_0]\n$$\n\nРаспишем компоненты этого выражения в терминах ковариации.\n\nПусть $X_1,\\ldots, X_n$ случайные величины, а $Y_1 = \\sum\\limits_{i=1}^n a_i X_i,\\; Y_2 = \\sum\\limits_{j=1}^m b_j X_j$ — их две произвольные линейные комбинации. Тогда:\n\n$$\n\\operatorname {Cov}[Y_1,Y_2] = \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^m a_i b_j \\operatorname {Cov}[X_i,X_j]$$.\n\n## Простой кригинг\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname{Cov}[Z^*, Z_0]\n$$\n\nРаспишем компоненты этого выражения в терминах ковариации.\n\n$\\operatorname{Var}[Z^*] = \\operatorname{Cov}[Z^*, Z^*] = \\operatorname{Cov}\\Big[\\sum_{i} \\lambda_i Z_i, \\sum_{j} \\lambda_j Z_j\\Big] = \\\\ \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\operatorname{Cov}[Z_i, Z_j] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}$\n\n$\\operatorname{Var}[Z_0] = \\operatorname{Cov}[Z_0, Z_0] = \\sigma_{00}$\n\n$\\operatorname{Cov}[Z^*, Z_0] = \\operatorname{Cov}\\Big[\\sum_{i} \\lambda_i Z_i, Z_0\\Big] =\\\\ \\sum_{i} \\lambda_i \\operatorname{Cov}[Z_i, Z_0] = \\sum_{i} \\lambda_i \\sigma_{i0}$\n\n## Простой кригинг\n\nТаким образом, выражение для ошибки\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname{Cov}[Z^*, Z_0]\n$$\n\nТрансформируется в\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nДля нахождения минимума этой квадратичной функции необходимо приравнять нулю ее производные по основной переменной $\\lambda$. Выберем в качестве «жертвы» коэффициенты с индексом $i$:\n\n$$\n\\frac{\\partial}{\\partial \\lambda_i} \\operatorname E\\big[(Z^* - Z_0)^2\\big] = 2 \\sum_{j} \\lambda_j \\sigma_{ij} - 2 \\sigma_{i0} = 0\n$$\n\n## Простой кригинг\n\n$$\n\\frac{\\partial}{\\partial \\lambda_i} \\operatorname E\\big[(Z^* - Z_0)^2\\big] = 2 \\sum_{j} \\lambda_j \\sigma_{ij} - 2 \\sigma_{i0} = 0\n$$\n\nТаким образом, система уравнений **простого кригинга** для точки $Z_0$ имеет вид:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}\\color{gray}{,~i = 1,...,N}}}}\n$$\n\n> Уравнения простого кригинга носят чисто теоретический характер. На практике используется метод обычного кригинга, в котором знание среднего значения случайной функции не требуется.\n\n## Дисперсия простого кригинга\n\nСуществует возможность оценить в каждой точке не только величину показателя, но также дисперсию оценки (в случае постоянного мат. ожидания — среднеквадратическую ошибку).\n\nДля этого необходимо коэффициенты $\\lambda_i$, полученные из системы уравнения простого кригинга\n\n$$\n\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}\n$$\n\nподставить в выражение среднеквадратической ошибки\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\n## Дисперсия простого кригинга\n\nУмножим обе части каждого уравнения простого кригинга на $\\lambda_i$ и просуммируем все уравнения по $i$:\n\n$$\n\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}~\\Bigg|\\times \\lambda_i\\\\\n\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} = \\sum_{i}\\lambda_i\\sigma_{i0}\n$$\n\nЗаметим, что левая часть уравнения присутствует в выражении среднеквадратической ошибки:\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\color{red}{\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\n## Дисперсия простого кригинга\n\nЗаменим $\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}$ на $\\sum_{i}\\lambda_i\\sigma_{i0}$ в выражении для СКО:\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\color{red}{\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00} =\\\\\n\\sum_{i}\\lambda_i\\sigma_{i0} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nОтсюда получаем выражение для дисперсии (ошибки) простого кригинга:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\sigma_{SK} = \\operatorname{Var}[Z^* - Z_0] = \\sigma_{00} - \\sum_{i}\\lambda_i\\sigma_{i0}}}}\n$$\n\n\n---\n\n## Стационарность приращений\n\nСтационарность второго порядка требует знания математического ожидания для вычисления ковариации. \nВ ряде случаев оценить математическое ожидание оказывается невозможно (оно может не существовать) или же оно действительно оказывается непостоянным.\n\nТогда пользуются еще более мягкой формой стационарности —  __стационарностью приращений__, при которой стационарной предполагается не сама с.ф. $Z(x)$, а производная от нее функция: \n\n  $$Y_h(x) = Z(x+h)-Z(x)$$\n\nФункция $Z(x)$, обладающая таким свойством, называется подчиняющейся _внутренней гипотезе_.\n\n\n## Стационарность приращений\n\nУ функции $Y_h(x) = Z(x+h)-Z(x)$ должны существовать математическое ожидание и дисперсия __приращений__:\n\n$$\n\\begin{cases}\n\\operatorname E[Z(x+h)-Z(x)] = \\langle a,h \\rangle \\\\\n\\operatorname {Var}[Z(x+h)-Z(x)] = 2\\gamma(h)\n\\end{cases}\n$$\n\n- $\\langle a,h \\rangle$ обозначает линейный тренд $a$ при заданном векторе $h$ (_математическое ожидание разности значений_), который выражается через скалярное произведение: $\\langle a,h \\rangle = \\sum_i a_i h_i$\n\n- $\\gamma(h)$ — дисперсия приращений, называемая _вариограммой_\n\n---\n\n## Стационарность приращений\n\n$$\n\\begin{cases}\n\\operatorname E[Z(x+h)-Z(x)] = \\langle a,h \\rangle \\\\\n\\operatorname {Var}[Z(x+h)-Z(x)] = 2\\gamma(h)\n\\end{cases}\n$$\n\nЕсли процесс подчиняется гипотезе стационарности второго рода $\\operatorname E[Z(x)] = m$, то $\\operatorname E[Z(x+h)-Z(x)] = \\operatorname E[Y_h(x)] = 0$ и вариограмму можно выразить следующим образом:\n\n$$\n2\\gamma(h) = \\operatorname {Var}[Z(x+h)-Z(x)] = \\operatorname {Var}[Y_h(x)] \\\\= \\operatorname E\\big[Y_h(x)\\big]^2 - \\Big(\\operatorname E\\big[Y_h(x)\\big]\\Big)^2 \\\\= \\operatorname E\\big[Y_h(x)\\big]^2 = \\operatorname E\\big[Z(x+h)-Z(x)\\big]^2\n$$\n\n## Стационарность приращений\n\nТаким образом, наиболее распространенная в геостатистике гипотеза подчиняется следующим условиям:\n\n$$\n\\begin{cases}\n\\operatorname E\\big[Z(x)\\big] = m\\\\\n\\operatorname E\\big[Z(x+h)-Z(x)\\big] = 0 \\\\\n\\operatorname E\\big[Z(x+h)-Z(x)\\big]^2 = 2\\gamma(h)\n\\end{cases}\n$$\n\n- Эти условия позволяют избавиться от необходимости знания среднего значения и дисперсии случайной функции и использовать для вычислений вариограмму. \n\n- Чтобы модифицировать соответствующим образом уравнения простого кригинга, необходимо знать связь между ковариацией и вариограммой.\n\n## Переход от ковариации к вариограмме\n\nРассмотрим ковариацию двух линейных комбинаций с.ф.:\n\n$$\n\\operatorname {Cov} \\Bigg[\\sum_{\\alpha=1}^N \\lambda_{\\alpha} Z(x_{\\alpha}), \\sum_{\\beta=1}^M \\mu_{\\beta} Z(x_{\\beta}) \\Bigg] = \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname C(x_{\\beta} - x_{\\alpha})\n$$\n\nИспользуя правило $\\operatorname {Cov}[X + \\alpha, Y + \\beta] = \\operatorname {Cov}[X, Y]$, введем условное начало координат:\n\n$$\n\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname C(x_{\\beta} - x_{\\alpha}) =\\\\\n=\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname {Cov} \\big[Z(x_{\\alpha}) - Z(x_0), Z(x_{\\beta}) - Z(x_0)\\big]\n$$\n\n## Переход от ковариации к вариограмме\n\nРаспишем ковариацию через математические ожидания, учитывая, что, согласно гипотезе, $\\operatorname E\\big[Z(x+h)-Z(x)\\big] = 0$:\n\n$$\n\\operatorname {Cov} \\big[Z(x_{\\alpha}) - Z(x_0), Z(x_{\\beta}) - Z(x_0)\\big] =\\\\\n= \\operatorname E\\big[Z_{\\alpha} - Z_0\\big]\\big[Z_{\\beta} - Z_0\\big] - \\operatorname E\\big[Z_{\\alpha} - Z_0\\big] \\operatorname E\\big[Z_{\\beta} - Z_0\\big] = \\\\\n= \\operatorname E\\big[Z_{\\alpha} - Z_0\\big]\\big[Z_{\\beta} - Z_0\\big]\n$$\n\nОбратим внимание, что произведение приращений можно выразить через квадраты приращений:\n\n$$\n\\color{blue}{(Z_{\\beta} - Z_{\\alpha})^2} = \\big[(Z_{\\beta} - Z_0) - (Z_{\\alpha} - Z_0)\\big]^2 = \\\\ = \\color{blue}{(Z_{\\alpha} - Z_0)^2} - 2\\color{red}{(Z_{\\alpha} - Z_0)(Z_{\\beta} - Z_0)} + \\color{blue}{(Z_{\\beta} - Z_0)^2}\n$$\n\n## Переход от ковариации к вариограмме\n\nИмеем:\n\n$$\n(Z_{\\alpha} - Z_0)(Z_{\\beta} - Z_0) = \\frac{1}{2} \\Big[(Z_{\\alpha} - Z_0)^2 + (Z_{\\beta} - Z_0)^2 - (Z_{\\beta} - Z_{\\alpha})^2\\Big]\n$$\n\nУчитывая, что $\\operatorname E\\big[Z(x+h)-Z(x)\\big]^2 = 2\\gamma(h)$, подставим это выражение в формулу вычисления ковариации:\n\n$$\n\\operatorname {Cov} \\big[Z(x_{\\alpha}) - Z(x_0), Z(x_{\\beta}) - Z(x_0)\\big] = \\operatorname E\\big[Z_{\\alpha} - Z_0\\big]\\big[Z_{\\beta} - Z_0\\big] = \\\\\n= \\frac{1}{2} \\operatorname E \\Big[(Z_{\\alpha} - Z_0)^2 + (Z_{\\beta} - Z_0)^2 - (Z_{\\beta} - Z_{\\alpha})^2\\Big] = \\\\\n= \\gamma(x_{\\alpha} - x_0) + \\gamma(x_{\\beta} - x_0) - \\gamma(x_{\\beta} - x_{\\alpha})\n$$\n\n## Переход от ковариации к вариограмме\n\nПодставим полученное выражение в двойную сумму:\n\n$$\n\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname {Cov} \\big[Z(x_{\\alpha}) - Z(x_0), Z(x_{\\beta}) - Z(x_0)\\big] = \\\\ \n= \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\big[\\gamma(x_{\\alpha} - x_0) + \\gamma(x_{\\beta} - x_0) - \\gamma(x_{\\beta} - x_{\\alpha})\\big] = \\\\\n= \\color{blue}{\\underbrace{\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\alpha} - x_0)}_0} + \\color{red}{\\underbrace{\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_0)}_0} - \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_{\\alpha}) = \\\\\n= - \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_{\\alpha}),\n$$\n\n## Переход от ковариации к вариограмме\n\nПодставим полученное выражение в двойную сумму:\n\n$$\n\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname {Cov} \\big[Z(x_{\\alpha}) - Z(x_0), Z(x_{\\beta}) - Z(x_0)\\big] = \\\\ \n= - \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_{\\alpha}),\n$$\n\n- $\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\alpha} - x_0) = \\color{blue}{\\underbrace{\\sum_{\\beta=1}^M \\mu_{\\beta}}_0} \\sum_{\\alpha=1}^N \\lambda_{\\alpha} \\gamma(x_{\\alpha} - x_0) = 0$\n\n- $\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_0) = \\color{red}{\\underbrace{\\sum_{\\alpha=1}^N \\lambda_{\\alpha}}_0} \\sum_{\\beta=1}^M \\mu_{\\beta} \\gamma(x_{\\beta} - x_0) = 0$\n\n## Переход от ковариации к вариограмме\n\nВ линейных комбинациях приращений сумма коэффициентов всегда равна $0$, поскольку это справедливо для каждого приращения. \n\nДля $\\alpha = 1$:\n\n$$\n\\lambda_1 (x_1 - x_0) = \\lambda_1 x_1 - \\lambda_1 x_0 = \\sum_{k=0}^1 \\rho_k x_k,\n$$\n\nгде $\\rho_0 = -\\lambda_1$, $\\rho_1 = \\lambda_1$. Следовательно:\n\n$$\n\\sum_{k=0}^1 \\rho_k = 0\n$$\n\n## Переход от ковариации к вариограмме\n\nСтационарный случай:\n\n$$\n\\operatorname {Cov} \\Bigg[\\sum_{\\alpha=1}^N \\lambda_{\\alpha} Z(x_{\\alpha}), \\sum_{\\beta=1}^M \\mu_{\\beta} Z(x_{\\beta}) \\Bigg] = \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname C(x_{\\beta} - x_{\\alpha})\n$$\n\nВнутренняя гипотеза:\n\n$$\n\\operatorname {Cov} \\Bigg[\\sum_{\\alpha=1}^N \\lambda_{\\alpha} Z(x_{\\alpha}), \\sum_{\\beta=1}^M \\mu_{\\beta} Z(x_{\\beta}) \\Bigg] = - \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_{\\alpha})\n$$\n\n> При соблюдении внутренней гипотезы в уравнениях кригинга можно принять $\\sigma_{\\alpha \\beta} = -\\gamma_{\\alpha \\beta}$\n\n## Обычный кригинг\n\nПусть дано неизвестное среднее $m = a_0$. Необходимо произвести линейную оценку $Z^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0$. Выразим среднюю квадратическую ошибку:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big] = \\operatorname {Var}[Z^* - Z_0] + \\big(E[Z^* - Z_0]\\big)^2 =\\\\\n= \\operatorname {Var}[Z^* - Z_0] + \\Bigg[\\lambda_0 + \\bigg(\\sum_i \\lambda_i - 1 \\bigg) a_0 \\Bigg]^2\n$$\n\n- Только компонента сдвига $\\operatorname E[Z^* - Z_0]$ содержит $\\lambda_0$, однако, в отличие от случая простого кригинга, мы не можем минимизировать ее, не зная $a_0$.\n- Единственный способ избавиться от $a_0$ заключается в том, чтобы наложить дополнительное условие $\\sum \\lambda_i - 1 = 0$\n\n## Обычный кригинг\n\nМинимизируем ранее введенную функцию ошибки:\n\n$$\n\\operatorname {Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nДля этого, с учетом дополнительного условия  $\\sum \\lambda_i - 1 = 0$ применим метод множителей Лагранжа и построим вспомогательную функцию:\n\n$$\nQ = \\operatorname {Var}[Z^* - Z_0] + 2\\mu \\bigg(\\sum_i \\lambda_i - 1 \\bigg),\n$$\n\nгде $\\mu$ -- неизвестный множитель Лагранжа.\n\n## Обычный кригинг\n\n$$\nQ = \\operatorname {Var}[Z^* - Z_0] + 2\\mu \\bigg(\\sum_i \\lambda_i - 1 \\bigg)$$\n\nДля минимизации функции приравняем нулю ее частные производные:\n\n$$\n\\begin{cases}\\frac{\\partial Q}{\\partial \\lambda_i} = 2 \\sum_j \\lambda_j \\sigma_{ij} - 2 \\sigma_{i0} + 2\\mu = 0,~i = 1,...,N,\\\\\n\\frac{\\partial Q}{\\partial \\mu} = 2\\bigg(\\sum_i \\lambda_i - 1 \\bigg) = 0\n\\end{cases}\n$$\n\n## Обычный кригинг\n\nИмеем $N + 1$ уравнений с $N + 1$ неизвестными:\n\n$$\n\\begin{cases}\\sum_j \\lambda_j \\sigma_{ij} + \\mu = \\sigma_{i0},~i = 1,...,N,\\\\\n\\sum_i \\lambda_i = 1\n\\end{cases}\n$$\n\nЗаменяя ковариацию на вариограмму, получаем __систему уравнений обычного кригинга__:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\begin{cases}\\sum_j \\lambda_j \\gamma_{ij} - \\mu = \\gamma_{i0},\\color{gray}{~i = 1,...,N,}\\\\\n\\sum_i \\lambda_i = 1\n\\end{cases}}}}\n$$\n\n> Это наиболее часто используемый в геостатистике метод оценки\n\n## Дисперсия обычного кригинга\n\nВывод формулы для оценки дисперсии обычного кригинга выполняется аналогично случаю простого кригинга. Умножим $N$ первых уравнений на $\\lambda_i$, просуммируем их по $i$:\n\n$$\n\\sum_j \\lambda_j \\gamma_{ij} - \\mu = \\gamma_{i0}~\\Bigg|\\times \\lambda_i\n$$\n\nУчтя дополнительное условие $\\sum_i \\lambda_i = 1$, получаем выражение для оценки дисперсии (ошибки) обычного кригинга:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\sigma_{OK} = \\operatorname {Var}[Z^* - Z_0] = \\sum_{i}\\lambda_i\\gamma_{i0} - \\mu}}}\n$$\n\n## Универсальный кригинг\n\nВ методе универсального кригинга осуществляется декомпозиция переменной $Z(x)$ в виде следующей суммы:\n\n$$\nZ(x) = m(x) + Y(x)\n$$\n\n- $m(x)$ — __дрифт__ (_drift_), гладкая детерминированная функция, описывающая _систематическую_ составляющую пространственной изменчивости явления;\n\n- $Y(x)$ - __остаток__ (_residual_), случайная функция с нулевым математическим ожиданием, описывающая _случайную_ составляющую пространственной изменчивости явления;\n\n> Декомпозиция любого явления на дрифт и остаток зависит от масштаба рассмотрения явления. \n\n## Универсальный кригинг\n\nМетод универсального кригинга используется, когда математическое ожидание случайного процесса непостоянно по пространству. Это позволяет интерполировать данные, в которых присутствует тренд.\n\nВ предположении, что м.о. имеет функциональную зависимость от других процессов в точке $x$, вводится следующая модель:\n\n$$\nm(x) = \\sum_{k=0}^{K} a_k f^k(x),\n$$\n\nгде $f^k(x)$ — известные _базисные функции_, а $a_k$ — фиксированные для точки $x$, но неизвестные коэффициенты.\n\n## Универсальный кригинг\n\n$$\nm(x) = \\sum_{k=0}^{K} a_k f^k(x),\n$$\n\n- Обычно первая базисная функция при $k = 0$ представляет собой константу, равную 1. Это позволяет включить в модель случай постоянного м.о.\n\n- Если среднее зависит только от местоположения, то оставшиеся функции $f^k(x), k > 0$, как правило, представляют собой одночлены от координат (например, для двумерного случая $f^2(p) = x^2 + y^2$)\n\n- Коэффициенты $a_k$ могут меняться в зависимости от $x$, но обязательно медленно, чтобы их можно было считать постоянными в окрестности $x$.\n\n## Универсальный кригинг\n\nВ качестве дрифта $m(x)$ можно использовать не только функцию от местоположения, но также значения внешней переменной — __ковариаты__.\n\nНапример, количество осадков можно связать с высотой точки $H(x)$ следующей моделью:\n\n$$\nZ(x) = a_0 + a_1 H(x) + Y(x)\n$$\n\nС статистической точки зрения это линейная регрессия, в которой остатки коррелированы (автокоррелированы).\n\n> В литературе данный метод называют также __регрессионным кригингом__ (_regression kriging_), а оценка дрифта — __пространственной регрессией__ (_spatial regression_).\n\n## Универсальный кригинг\n\nДля вывода уравнений рассмотрим среднеквадратическую ошибку:\n\n$$\n\\operatorname E[Z^* - Z_0]^2 = \\operatorname {Var}[Z^* - Z_0] + \\big(\\operatorname E[Z^* - Z_0]\\big)^2\n$$\n\nИспользуя введенную модель дрифта $m(x) = \\sum_{k=0}^{K} a^k f^k(x)$ распишем выражение для мат.ожидания приращений:\n\n$$\n\\operatorname E[Z^* - Z_0] = \\operatorname E[Z^*\\big] - \\operatorname E[Z_0] = \\\\\n\\sum_i \\lambda_i \\sum_k a_k f_i^k - \\sum_k a_k f_0^k = \\sum_k a_k \\Bigg(\\sum_i \\lambda_i f_i^k - f_0^k\\Bigg)\n$$\n\n## Универсальный кригинг\n\n$$\n\\operatorname E[Z^* - Z_0] = \\sum_k a_k \\Bigg(\\sum_i \\lambda_i f_i^k - f_0^k\\Bigg)\n$$\n\nЧтобы минимизировать $\\operatorname E[Z^* - Z_0]$ независимо от коэффициентов $a_k$, достаточно в вышеприведенной формуле приравнять нулю выражение в скобках. Отсюда имеем:\n\n$$\\sum_i \\lambda_i f_i^k = f_0^k,~k = 0, 1, ..., K.$$\n\nЭти условия называются __условиями универсальности__. Отсюда идет название метода — _универсальный кригинг_\n\n> Условия универсальности гаранируют, что оценка $Z^*$ является __несмещенной__ для _любых_ значений $a_k$.\n\n## Универсальный кригинг\n\nМинимизируем ранее введенную функцию ошибки:\n\n$$\n\\operatorname {Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nДля этого, с учетом дополнительного условия  $\\sum_i \\lambda_i f_i^k = f_0^k$ применим метод множителей Лагранжа и построим вспомогательную функцию:\n\n$$Q = \\operatorname {Var}[Z^* - Z_0] + 2 \\sum_{k=0}^K \\mu_k \\Bigg[ \\sum_i \\lambda_i f_i^k - f_0^k\\Bigg],$$\n\nгде $\\mu_k,~k = 0, 1, ..., K$ представляют $K + 1$ дополнительных неизвестных, множители Лагранжа.\n\n## Универсальный кригинг\n\n$$\n\\operatorname {Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nДля минимизации функции приравняем нулю ее частные производные:\n\n$$\n\\begin{cases}\\frac{\\partial Q}{\\partial \\lambda_i} = 2 \\sum_j \\lambda_j \\sigma_{ij} -2 \\sigma_{i0} + 2 \\sum_k \\mu_k f_i^k = 0,\\color{gray}{~i = 1,...,N,}\\\\\n\\frac{\\partial Q}{\\partial \\mu} = 2\\bigg[\\sum_i \\lambda_i f_i^k - f_0^k \\bigg] = 0\\color{gray}{,~k = 0, 1,..., K.}\n\\end{cases}\n$$\n\n## Универсальный кригинг\n\nИмеем систему из $N + K + 1$ уравнений с $N + K + 1$ неизвестными:\n\n$$\n\\begin{cases}\\sum_j \\lambda_j \\sigma_{ij} + \\sum_k \\mu_k f_i^k = \\sigma_{i0},~i = 1,...,N,\\\\\n\\sum_i \\lambda_i f_i^k = f_0^k,~k = 0, 1,..., K.\n\\end{cases}\n$$\n\nЗаменяя ковариацию на вариограмму, получаем __систему уравнений универсального кригинга__:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\begin{cases}\\sum_j \\lambda_j \\gamma_{ij} - \\sum_k \\mu_k f_i^k = \\gamma_{i0},\\color{gray}{~i = 1,...,N,}\\\\\n\\sum_i \\lambda_i f_i^k = f_0^k\\color{gray}{,~k = 0, 1,..., K.}\n\\end{cases}}}}\n$$\n\n## Дисперсия универсального кригинга\n\nВывод формулы для оценки дисперсии универсального кригинга выполняется аналогично случаю обычного кригинга. Умножим $N$ первых уравнений на $\\lambda_i$, просуммируем их по $i$:\n\n$$\n\\sum_j \\lambda_j \\gamma_{ij} - \\sum_k \\mu_k f_i^k = \\gamma_{i0} ~ \\Bigg|\\times \\lambda_i\n$$\n\nУчтя дополнительное условие $\\sum_i \\lambda_i f_i^k = f_0^k$, получаем выражение для оценки дисперсии (ошибки) универсального кригинга:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\sigma^2_{UK} = E[Z^* - Z_0]^2 = \\sum_{i}\\lambda_i\\gamma_{i0} - \\sum_k \\mu_k f_0^k}}}\n$$\n\n## Библиография\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}