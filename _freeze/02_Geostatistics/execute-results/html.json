{
  "hash": "7efe1910bf93c57e5f4caac1fbc82aea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Геостатистика\"\nsubtitle: \"Пространственная статистика\"\ndate: today\ndate-format: long\nauthor: \"Самсонов Тимофей Евгеньевич\"\nexecute:\n  echo: false\n  freeze: true\nengine: knitr\nformat:\n  revealjs: \n    theme: [default, custom.scss]\n    margin: 0.2\n    width: 1280\n    height: 720\n    slide-number: true\n    footer: \"Самсонов Т. Е. Пространственная статистика: курс лекций\"\n    header-includes: <link rel=\"stylesheet\" media=\"screen\" href=\"https://fontlibrary.org//face/pt-sans\" type=\"text/css\"/>\nbibliography: references.yaml\nmainfont: PT Sans\n---\n\n\n## Базовые компоненты\n\n1.  Пространственные локации (точки) $$\\{p_1, p_2, ..., p_n\\}$$\n2.  Данные в этих локациях $$\\{Z(p_1), Z(p_2), ..., Z(p_n)\\}$$\n\n> Обе компоненты в общем случае являются случайными\n\n## Случайная величина\n\n> **Случайной величиной** $Z(w)$ называется функция, которая в результате случайного события $w$ принимает некоторое вещественнозначное значение.\n\n::: columns\n::: {.column width=\"50%\"}\nНапример, при анализе температуры водоема в отдельно взятой точке в толще воды случайной величиной (функцией) является собственно температура, а событием — та совокупность физико-химических условий, которая сложилась в данной точке в момент измерений.\n:::\n\n::: {.column width=\"50%\"}\n*Элемент случайности вносится именно событием*, которое в природе может быть чрезвычайно сложной и трудно предсказуемой комбинацией факторов, в то время как *случайная величина связана с событием функциональной зависимостью*.\n:::\n:::\n\n## Пространственная модель\n\n$p \\in \\mathbb{R}^k$ — точка в $k$-мерном Евклидовом пространстве\n\n$Z(p)$ — **случайная величина** в точке $p$\n\nЕсли $p$ меняется над индексным множеством $D \\subset \\mathbb{R}^k$, то формируется **случайный процесс**:\n\n$$\n\\{Z(p) | p \\in D\\}\n$$\n\nРезультат наблюдения случайного процесса в точках $D$ является **реализацией** случайного процесса:\n\n$$\n\\{z(p) | p \\in D\\}\n$$\n\n> В общем случае $D$ и $Z$ случайны и независимы\n\n## Случайный процесс (функция)\n\n> **Случайный процесс** — это семейство случайных величин, индексированных некоторым параметром $t$\n\n-   Наиболее часто анализируются одномерные случайные процессы, в которых $t$ — это время\n\n-   Пример случайного процесса — температура не в один момент времени, а в течение некоторого промежутка времени\n\n-   Пространственная статистика изучает случайные процессы, в которых $t$ — это координата точки (обычно на плоскости)\n\n## Случайный процесс (функция)\n\n-   В каждой точке $p_i$ существует некоторая *случайная величина* $Z(p_i)$ — **сечение случайного процесса**\n\n-   При изменении точки $p_i$ наблюдаемое значение случайного процесса меняется случайным образом, поскольку определяется оно не только местоположением, но и заранее неизвестным случайным событием\n\n-   Большинство природных явлений показывают зависимость наблюдаемых значений от их взаимного местоположения, которая проявляется в наличии корреляции значений $Z(\\mathbf{p})$ и $Z(\\mathbf{p} + \\mathbf{h})$, где $\\mathbf{h}$ — вектор смещения между точками\n\n-   Тем короче $h$, тем, как правило, сильнее выражена корреляция значений\n\n## Автокорреляция\n\n::: columns\n::: {.column width=\"40%\"}\n![](images/Student.jpg){width=\"100%\"}\n:::\n\n::: {.column width=\"60%\"}\nСтьюдент в письме Карлу Пирсону (1900):\n\n> *...В целом, корреляция ослабевает, если охват по времени или пространству увеличивается. Меня не покидает мысль, что было бы великим достижением установить закон, согласно которому корреляция будет ослабевать с увеличением охвата*\n:::\n:::\n\n## Автокорреляция\n\nБританский статистик и биолог **Рональд Эйлмер Фишер** изучал пространственное распределение характеристик растений на опытных площадках, будучи сотрудником Ротамстедской агрохимической станции в 1920-1930-е гг.\n\n::: columns\n::: {.column width=\"40%\"}\n![Sir Ronald Aylmer Fisher (1876-1937)](images/Fisher.png){width=\"70%\"}\n:::\n\n::: {.column width=\"60%\"}\nСтьюдент в письме Карлу Пирсону (1900):\n\n> ...всестороннее подтверждение получил тот факт, что близко расположенные участки более схожи, чем удаленные, судя по данным об урожайности... следовательно, наиболее верным будет делать \\[выборочные\\] блоки максимально компактными\\_\n>\n> <footer>--- «The Design of Experiments» (1935)</footer>\n:::\n:::\n\n## Пространственная статистика\n\n1.  **Геостатистика** *(geostatistics)*\n\n    -   $D$ — фиксированное подмножество в $\\mathbb{R}^k$\n    -   $Z(p)$ — случайный вектор в каждой точке $p$\n    -   Исследуется пространственное распределение\n\n2.  **Сеточные данные** *(lattice data)*\n\n    -   $D$ — фиксированное счетное подмножество точек $\\mathbb{R}^k$\n    -   $Z(p)$ — случайный вектор в каждой точке $p$\n    -   Исследуется пространственная зависимость и гетерогенность\n\n3.  **Конфигурации точек** *(point patterns)*\n\n    -   $D$ — счетное подмножество точек $\\mathbb{R}^k$ *(точечный процесс)*\n    -   $Z(p)$ — константа или счетное множество *(маркированный точечный процесс)*\n    -   Исследуется пространственное размещение\n\n## Математическое ожидание\n\n**Математическое ожидание** — наиболее вероятная реализация случайного процесса:\n\n$$\n\\operatorname E[Z(p)]=m(p)\n$$\n\n-   Пусть дан географический регион, в котором производятся наблюдения температуры в течение месяца.\n\n-   В каждый момент времени мы имеем непрерывное поле температуры — реализацию случайного процесса и относящиеся к ней данные наблюдений на метеостанциях\n\n-   Осреднив данные за период наблюдений восстановим выборочное среднее поле распределения температур — оценку математического ожидания случайного процесса\n\n## Дисперсия\n\n**Дисперсия** — мера разброса реализаций случайного процесса относительно его математического ожидания:\n\n$$\n\\operatorname {Var}[Z(p)]= \\operatorname E[Z^2(p)]-m^2(p)\n$$\n\n> Аналогично математическому ожиданию, дисперсия двумерного с.п. представляет собой *поле распределения*, значение которого в каждой точке равно дисперсии локального сечения с.п.\n\n## Ковариация\n\n**Ковариация** — мера линейной зависимости сечений случайного процесса в двух точках $p_1$ и $p_2$:\n\n$$\n\\operatorname {Cov}(p_1,p_2) = \\operatorname {Cov}[Z(p_1), Z(p_2)] = \\\\\\operatorname E[Z(p_1)Z(p_2)]-m(p_1)m(p_2)\n$$\n\n> Недостатком ковариации является необходимость знания математического ожидания с.п. Это условие выполняется далеко не всегда, что связано с тем что как правило приходится иметь дело только с одной реализацией с.п\n\n## Свойства моментов случайных процессов\n\n-   Моменты пространственных случайных процессов являются **функциями**, а не *константами*, в отличие от моментов случайных величин.\n\n-   Давать оценку пространственной структуре явления на основе вычисленных моментов с.п. можно только при условии, что он удовлетворяет свойствам **стационарности** и **эргодичности.**\n\n## Гипотеза стационарности\n\n-   Функции корреляции между данными зависят только от взаимного расположения точек измерений, а не от их конкретного местоположения в пространстве.\n\n-   В этом случае пространственная корреляция определяется вектором $\\mathbf{h}$ между точками.\n\n-   Для изотропного случая, когда корреляция не зависит от направления, а только от расстояния вектор $\\mathbf{h}$ переходит в скаляр — расстояние $d$.\n\n## Стационарность\n\n**Стационарность** *в строгом смысле* означает что функция распределения множества случайных величин для любой комбинации точек ${x_1, x_2,...,x_k}$ и любого $k < \\infty$ остается неизменной при смещении этой комбинации на произвольный вектор $h$:\n\n$$\n\\operatorname P\\{Z(x_1)<z_1,...,Z(x_k)<z_k\\} = \\\\ \\operatorname P\\{Z(x_1 + h)<z_1,...,Z(x_k + h)<z_k\\}\n$$\n\n## Стационарность\n\n-   Стационарность по другому называют **однородностью в пространстве**, подразумевая что явление ведет себя одинаковым образом в любой точке пространства, как бы повторяет само себя.\n\n-   Если с.ф. стационарна, все ее моменты будут инвариантны относительно сдвигов (то есть будут постоянны), а это означает что для их оценки можно использовать ограниченную в пространстве область.\n\n-   В реальности же подобного рода «идеальное» поведение встречается крайне редко, поэтому используют более слабое предположение о стационарности второго порядка.\n\n## Стационарность второго порядка\n\nСлучайная функция имеет имеет **стационарность второго порядка**, если для любых точек $x$ и $x+h$ в $R^k$\n\n$$\n\\begin{cases}\n  \\operatorname E[Z(x)] = m \\\\\n  \\operatorname E[(Z(x)-m)(Z(x+h)-m)] = \\operatorname C(h)\n\\end{cases}\n$$\n\nМатематическое ожидание с.ф. постоянно, а ковариация зависит только от вектора $h$ между точками и не зависит от их абсолютного положения.\n\nЕсли ковариация также не зависит от направления, а только от расстояния между точками, то $h$ вырождается в скаляр, а такая случайная функция является *изотропной стационарной*.\n\n## Эргодичность\n\nСтационарная случайная функция $Z(x,w)$ называется **эргодической**, если ее среднее по области $V \\subset R^k$ сходится к математическому ожиданию $m(w)$ при стремлении $V$ к бесконечности:\n\n$$\n\\lim_{V \\rightarrow \\infty} \\frac{1}{|V|}\\int_{V} Z(x,w)dx = m(w)\n$$\n\n$|V|$ обозначает *меру* области $V$ (площадь, объем). Предполагается что сама область $V$ растет во всех направлениях, и предел ее роста не зависит от ее формы.\n\n> Cреднее по всем возможным реализациям равно среднему отдельной безграничной в пространстве реализации.\n\n## Эргодичность\n\n-   Дан кувшин с песком, в котором необходимо определить долю объема, занятую содержимым.\n\n-   Зафиксируем некоторую точку $x$ в системе отсчета, привязанной к кувшину, и будем его встряхивать бесконечное число раз, каждый раз фиксируя, оказалась ли точка $x$ внутри песчинки (записываем 1) или же попала в свободное между ними пространство (записываем 0)\n\n-   Из серии подобных экспериментов мы сможем оценить среднее значение индикаторной функции $\\operatorname I(x,w)$, которое равно вероятности попадания зерна в точку $x$, и которое не зависит от $x$.\n\n-   Эта вероятность и будет равна доли объема кувшина, занятой песком.\n\n## Эргодичность\n\n-   Аналогичный результат можно получить, если теперь зафиксировать кувшин, а точку $x$ выбирать каждый раз случайным образом.\n\n-   В первом случае берется среднее по реализациям, а во втором — среднее по пространству.\n\n> В реальных экспериментах приходится иметь дело со вторым случаем.\n\n## Простой кригинг\n\nДля оценки в точке $z_0 = z(p_0)$ по $N$ измерениям $z_1, ..., z_N$ ищутся коэффициенты следующего выражения:\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0\n$$\n\nКонстанта $\\lambda_0$ и веса $\\lambda_i$ подобираются таким образом, что минимизируется среднеквадратическая ошибка:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big],\n$$ то есть математическое ожидание квадрата отклонения оценки от реального значения в точке $p_0$.\n\n## Простой кригинг\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0\n$$\n\nИспользуя соотношение $\\operatorname {Var}[X] = \\operatorname E[X^2] - (\\operatorname E[X])^2$, можно выразить среднюю квадратическую ошибку как:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big] = \\operatorname{Var}[Z^* - Z_0] + (\\operatorname E[Z^* - Z_0])^2\n$$\n\nПоскольку дисперсия нечувствительна к сдвигам, изменение константы $\\lambda_0$ влияет только на компоненту $\\operatorname E[Z^* - Z_0]$. Приравняем ее нулю:\n\n$$\n\\operatorname E[Z^* - Z_0] = \\operatorname E\\Big[\\sum_{i} \\lambda_i Z_i + \\lambda_0 - Z_0\\Big] = 0\n$$\n\n## Простой кригинг\n\n$$\n\\operatorname E[Z^* - Z_0] = \\operatorname E\\Big[\\sum_{i} \\lambda_i Z_i + \\lambda_0 - Z_0\\Big] = 0\n$$\n\nПоскольку $\\lambda_0$ явялется константой, то по свойству мат. ожидания ее можно вынести за скобки:\n\n$$\n\\lambda_0 = -\\operatorname E\\Big[\\sum_{i} \\lambda_i Z_i - Z_0\\Big] = m_0 - \\sum_i \\lambda_i m_i,\n$$\n\nгде $m_i$ — известные значения мат. ожиданий случайной функции в каждой точке исходных данных, $m_0$ — известное мат. ожидание в интерполируемой точке.\n\n## Простой кригинг\n\nИмея:\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0,\\\\\n\\lambda_0 = m_0 - \\sum_i \\lambda_i m_i,\n$$\n\nПолучаем:\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + m_0 - \\sum_i \\lambda_i m_i = \\\\\nm_0 + \\sum_{i} \\lambda_i (Z_i - m_i)\n$$\n\n## Простой кригинг\n\n$$\nZ^* = m_0 + \\sum_{i} \\lambda_i (Z_i - m_i)\n$$\n\nПоскольку константа $m_0$ известна заранее, задачу оценки можно выполнить для переменной $Y(p) = Z(p) - m(p)$, используя линейную оценку\n\n$$\nY^* = \\sum_{i} \\lambda_i Y_i,\n$$\n\nи прибавляя к полученному результату $m_0$.\n\nОсновной вопрос заключается в нахождении коэффициентов $\\lambda_i$.\n\n## Простой кригинг\n\nПоскольку мы показали, что компонента $\\operatorname E[Z^* - Z_0]$ может быть приравнена нулю, среднеквадратическая ошибка равна дисперсии:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big] = \\operatorname{Var}[Z^* - Z_0]\n$$\n\nИспользуя свойства:\n\n-   $\\operatorname{Var}[X + Y] = \\operatorname{Var}[X] + \\operatorname{Var}[Y] + 2 \\operatorname{Cov}[X, Y]$,\\\n-   $\\operatorname{Var}[-X] = \\operatorname{Var}[X]$,\n-   $\\operatorname{Cov}[X, -Y] = -\\operatorname{Cov}[X, Y]$, получаем:\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname{Cov}[Z^*, Z_0]\n$$\n\n## Простой кригинг\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname {Cov}[Z^*, Z_0]\n$$\n\nРаспишем компоненты этого выражения в терминах ковариации.\n\nПусть $X_1,\\ldots, X_n$ случайные величины, а $Y_1 = \\sum\\limits_{i=1}^n a_i X_i,\\; Y_2 = \\sum\\limits_{j=1}^m b_j X_j$ — их две произвольные линейные комбинации. Тогда:\n\n$$\n\\operatorname {Cov}[Y_1,Y_2] = \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^m a_i b_j \\operatorname {Cov}[X_i,X_j]$$.\n\n## Простой кригинг\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname{Cov}[Z^*, Z_0]\n$$\n\nРаспишем компоненты этого выражения в терминах ковариации.\n\n$\\operatorname{Var}[Z^*] = \\operatorname{Cov}[Z^*, Z^*] = \\operatorname{Cov}\\Big[\\sum_{i} \\lambda_i Z_i, \\sum_{j} \\lambda_j Z_j\\Big] = \\\\ \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\operatorname{Cov}[Z_i, Z_j] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}$\n\n$\\operatorname{Var}[Z_0] = \\operatorname{Cov}[Z_0, Z_0] = \\sigma_{00}$\n\n$\\operatorname{Cov}[Z^*, Z_0] = \\operatorname{Cov}\\Big[\\sum_{i} \\lambda_i Z_i, Z_0\\Big] =\\\\ \\sum_{i} \\lambda_i \\operatorname{Cov}[Z_i, Z_0] = \\sum_{i} \\lambda_i \\sigma_{i0}$\n\n## Простой кригинг\n\nТаким образом, выражение для ошибки\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname{Cov}[Z^*, Z_0]\n$$\n\nТрансформируется в\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nДля нахождения минимума этой квадратичной функции необходимо приравнять нулю ее производные по основной переменной $\\lambda$. Выберем в качестве «жертвы» коэффициенты с индексом $i$:\n\n$$\n\\frac{\\partial}{\\partial \\lambda_i} \\operatorname E\\big[(Z^* - Z_0)^2\\big] = 2 \\sum_{j} \\lambda_j \\sigma_{ij} - 2 \\sigma_{i0} = 0\n$$\n\n## Простой кригинг\n\n$$\n\\frac{\\partial}{\\partial \\lambda_i} \\operatorname E\\big[(Z^* - Z_0)^2\\big] = 2 \\sum_{j} \\lambda_j \\sigma_{ij} - 2 \\sigma_{i0} = 0\n$$\n\nТаким образом, система уравнений **простого кригинга** для точки $Z_0$ имеет вид:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}\\color{gray}{,~i = 1,...,N}}}}\n$$\n\n> Уравнения простого кригинга носят чисто теоретический характер. На практике используется метод обычного кригинга, в котором знание среднего значения случайной функции не требуется.\n\n## Дисперсия простого кригинга\n\nСуществует возможность оценить в каждой точке не только величину показателя, но также дисперсию оценки (в случае постоянного мат. ожидания — среднеквадратическую ошибку).\n\nДля этого необходимо коэффициенты $\\lambda_i$, полученные из системы уравнения простого кригинга\n\n$$\n\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}\n$$\n\nподставить в выражение среднеквадратической ошибки\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\n## Дисперсия простого кригинга\n\nУмножим обе части каждого уравнения простого кригинга на $\\lambda_i$ и просуммируем все уравнения по $i$:\n\n$$\n\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}~\\Bigg|\\times \\lambda_i\\\\\n\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} = \\sum_{i}\\lambda_i\\sigma_{i0}\n$$\n\nЗаметим, что левая часть уравнения присутствует в выражении среднеквадратической ошибки:\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\color{red}{\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\n## Дисперсия простого кригинга\n\nЗаменим $\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}$ на $\\sum_{i}\\lambda_i\\sigma_{i0}$ в выражении для СКО:\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\color{red}{\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00} =\\\\\n\\sum_{i}\\lambda_i\\sigma_{i0} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nОтсюда получаем выражение для дисперсии (ошибки) простого кригинга:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\sigma_{SK} = \\operatorname{Var}[Z^* - Z_0] = \\sigma_{00} - \\sum_{i}\\lambda_i\\sigma_{i0}}}}\n$$\n\n------------------------------------------------------------------------\n\n## Стационарность приращений\n\nСтационарность второго порядка требует знания математического ожидания для вычисления ковариации. В ряде случаев оценить математическое ожидание оказывается невозможно (оно может не существовать) или же оно действительно оказывается непостоянным.\n\nТогда пользуются еще более мягкой формой стационарности — **стационарностью приращений**, при которой стационарной предполагается не сама с.ф. $Z(x)$, а производная от нее функция:\n\n$$Y_h(x) = Z(x+h)-Z(x)$$\n\nФункция $Z(x)$, обладающая таким свойством, называется подчиняющейся *внутренней гипотезе*.\n\n## Стационарность приращений\n\nУ функции $Y_h(x) = Z(x+h)-Z(x)$ должны существовать математическое ожидание и дисперсия **приращений**:\n\n$$\n\\begin{cases}\n\\operatorname E[Z(x+h)-Z(x)] = \\langle a,h \\rangle \\\\\n\\operatorname {Var}[Z(x+h)-Z(x)] = 2\\gamma(h)\n\\end{cases}\n$$\n\n-   $\\langle a,h \\rangle$ обозначает линейный тренд $a$ при заданном векторе $h$ (*математическое ожидание разности значений*), который выражается через скалярное произведение: $\\langle a,h \\rangle = \\sum_i a_i h_i$\n\n-   $\\gamma(h)$ — дисперсия приращений, называемая *вариограммой*\n\n------------------------------------------------------------------------\n\n## Стационарность приращений\n\n$$\n\\begin{cases}\n\\operatorname E[Z(x+h)-Z(x)] = \\langle a,h \\rangle \\\\\n\\operatorname {Var}[Z(x+h)-Z(x)] = 2\\gamma(h)\n\\end{cases}\n$$\n\nЕсли процесс подчиняется гипотезе стационарности второго рода $\\operatorname E[Z(x)] = m$, то $\\operatorname E[Z(x+h)-Z(x)] = \\operatorname E[Y_h(x)] = 0$ и вариограмму можно выразить следующим образом:\n\n$$\n2\\gamma(h) = \\operatorname {Var}[Z(x+h)-Z(x)] = \\operatorname {Var}[Y_h(x)] \\\\= \\operatorname E\\big[Y_h(x)\\big]^2 - \\Big(\\operatorname E\\big[Y_h(x)\\big]\\Big)^2 \\\\= \\operatorname E\\big[Y_h(x)\\big]^2 = \\operatorname E\\big[Z(x+h)-Z(x)\\big]^2\n$$\n\n## Стационарность приращений\n\nТаким образом, наиболее распространенная в геостатистике гипотеза подчиняется следующим условиям:\n\n$$\n\\begin{cases}\n\\operatorname E\\big[Z(x)\\big] = m\\\\\n\\operatorname E\\big[Z(x+h)-Z(x)\\big] = 0 \\\\\n\\operatorname E\\big[Z(x+h)-Z(x)\\big]^2 = 2\\gamma(h)\n\\end{cases}\n$$\n\n-   Эти условия позволяют избавиться от необходимости знания среднего значения и дисперсии случайной функции и использовать для вычислений вариограмму.\n\n-   Чтобы модифицировать соответствующим образом уравнения простого кригинга, необходимо знать связь между ковариацией и вариограммой.\n\n## Переход от ковариации к вариограмме\n\nРассмотрим ковариацию двух линейных комбинаций с.ф.:\n\n$$\n\\operatorname {Cov} \\Bigg[\\sum_{\\alpha=1}^N \\lambda_{\\alpha} Z(x_{\\alpha}), \\sum_{\\beta=1}^M \\mu_{\\beta} Z(x_{\\beta}) \\Bigg] = \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname C(x_{\\beta} - x_{\\alpha})\n$$\n\nИспользуя правило $\\operatorname {Cov}[X + \\alpha, Y + \\beta] = \\operatorname {Cov}[X, Y]$, введем условное начало координат:\n\n$$\n\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname C(x_{\\beta} - x_{\\alpha}) =\\\\\n=\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname {Cov} \\big[Z(x_{\\alpha}) - Z(x_0), Z(x_{\\beta}) - Z(x_0)\\big]\n$$\n\n## Переход от ковариации к вариограмме\n\nРаспишем ковариацию через математические ожидания, учитывая, что, согласно гипотезе, $\\operatorname E\\big[Z(x+h)-Z(x)\\big] = 0$:\n\n$$\n\\operatorname {Cov} \\big[Z(x_{\\alpha}) - Z(x_0), Z(x_{\\beta}) - Z(x_0)\\big] =\\\\\n= \\operatorname E\\big[Z_{\\alpha} - Z_0\\big]\\big[Z_{\\beta} - Z_0\\big] - \\operatorname E\\big[Z_{\\alpha} - Z_0\\big] \\operatorname E\\big[Z_{\\beta} - Z_0\\big] = \\\\\n= \\operatorname E\\big[Z_{\\alpha} - Z_0\\big]\\big[Z_{\\beta} - Z_0\\big]\n$$\n\nОбратим внимание, что произведение приращений можно выразить через квадраты приращений:\n\n$$\n\\color{blue}{(Z_{\\beta} - Z_{\\alpha})^2} = \\big[(Z_{\\beta} - Z_0) - (Z_{\\alpha} - Z_0)\\big]^2 = \\\\ = \\color{blue}{(Z_{\\alpha} - Z_0)^2} - 2\\color{red}{(Z_{\\alpha} - Z_0)(Z_{\\beta} - Z_0)} + \\color{blue}{(Z_{\\beta} - Z_0)^2}\n$$\n\n## Переход от ковариации к вариограмме\n\nИмеем:\n\n$$\n(Z_{\\alpha} - Z_0)(Z_{\\beta} - Z_0) = \\frac{1}{2} \\Big[(Z_{\\alpha} - Z_0)^2 + (Z_{\\beta} - Z_0)^2 - (Z_{\\beta} - Z_{\\alpha})^2\\Big]\n$$\n\nУчитывая, что $\\operatorname E\\big[Z(x+h)-Z(x)\\big]^2 = 2\\gamma(h)$, подставим это выражение в формулу вычисления ковариации:\n\n$$\n\\operatorname {Cov} \\big[Z(x_{\\alpha}) - Z(x_0), Z(x_{\\beta}) - Z(x_0)\\big] = \\operatorname E\\big[Z_{\\alpha} - Z_0\\big]\\big[Z_{\\beta} - Z_0\\big] = \\\\\n= \\frac{1}{2} \\operatorname E \\Big[(Z_{\\alpha} - Z_0)^2 + (Z_{\\beta} - Z_0)^2 - (Z_{\\beta} - Z_{\\alpha})^2\\Big] = \\\\\n= \\gamma(x_{\\alpha} - x_0) + \\gamma(x_{\\beta} - x_0) - \\gamma(x_{\\beta} - x_{\\alpha})\n$$\n\n## Переход от ковариации к вариограмме\n\nПодставим полученное выражение в двойную сумму:\n\n$$\n\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname {Cov} \\big[Z(x_{\\alpha}) - Z(x_0), Z(x_{\\beta}) - Z(x_0)\\big] = \\\\ \n= \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\big[\\gamma(x_{\\alpha} - x_0) + \\gamma(x_{\\beta} - x_0) - \\gamma(x_{\\beta} - x_{\\alpha})\\big] = \\\\\n= \\color{blue}{\\underbrace{\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\alpha} - x_0)}_0} + \\color{red}{\\underbrace{\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_0)}_0} - \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_{\\alpha}) = \\\\\n= - \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_{\\alpha}),\n$$\n\n## Переход от ковариации к вариограмме\n\nПодставим полученное выражение в двойную сумму:\n\n$$\n\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname {Cov} \\big[Z(x_{\\alpha}) - Z(x_0), Z(x_{\\beta}) - Z(x_0)\\big] = \\\\ \n= - \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_{\\alpha}),\n$$\n\n-   $\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\alpha} - x_0) = \\color{blue}{\\underbrace{\\sum_{\\beta=1}^M \\mu_{\\beta}}_0} \\sum_{\\alpha=1}^N \\lambda_{\\alpha} \\gamma(x_{\\alpha} - x_0) = 0$\n\n-   $\\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_0) = \\color{red}{\\underbrace{\\sum_{\\alpha=1}^N \\lambda_{\\alpha}}_0} \\sum_{\\beta=1}^M \\mu_{\\beta} \\gamma(x_{\\beta} - x_0) = 0$\n\n## Переход от ковариации к вариограмме\n\nВ линейных комбинациях приращений сумма коэффициентов всегда равна $0$, поскольку это справедливо для каждого приращения.\n\nДля $\\alpha = 1$:\n\n$$\n\\lambda_1 (x_1 - x_0) = \\lambda_1 x_1 - \\lambda_1 x_0 = \\sum_{k=0}^1 \\rho_k x_k,\n$$\n\nгде $\\rho_0 = -\\lambda_1$, $\\rho_1 = \\lambda_1$. Следовательно:\n\n$$\n\\sum_{k=0}^1 \\rho_k = 0\n$$\n\n## Переход от ковариации к вариограмме\n\nСтационарный случай:\n\n$$\n\\operatorname {Cov} \\Bigg[\\sum_{\\alpha=1}^N \\lambda_{\\alpha} Z(x_{\\alpha}), \\sum_{\\beta=1}^M \\mu_{\\beta} Z(x_{\\beta}) \\Bigg] = \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\operatorname C(x_{\\beta} - x_{\\alpha})\n$$\n\nВнутренняя гипотеза:\n\n$$\n\\operatorname {Cov} \\Bigg[\\sum_{\\alpha=1}^N \\lambda_{\\alpha} Z(x_{\\alpha}), \\sum_{\\beta=1}^M \\mu_{\\beta} Z(x_{\\beta}) \\Bigg] = - \\sum_{\\alpha=1}^N \\sum_{\\beta=1}^M \\lambda_{\\alpha} \\mu_{\\beta} \\gamma(x_{\\beta} - x_{\\alpha})\n$$\n\n> При соблюдении внутренней гипотезы в уравнениях кригинга можно принять $\\sigma_{\\alpha \\beta} = -\\gamma_{\\alpha \\beta}$\n\n## Обычный кригинг\n\nПусть дано неизвестное среднее $m = a_0$. Необходимо произвести линейную оценку $Z^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0$. Выразим среднюю квадратическую ошибку:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big] = \\operatorname {Var}[Z^* - Z_0] + \\big(E[Z^* - Z_0]\\big)^2 =\\\\\n= \\operatorname {Var}[Z^* - Z_0] + \\Bigg[\\lambda_0 + \\bigg(\\sum_i \\lambda_i - 1 \\bigg) a_0 \\Bigg]^2\n$$\n\n-   Только компонента сдвига $\\operatorname E[Z^* - Z_0]$ содержит $\\lambda_0$, однако, в отличие от случая простого кригинга, мы не можем минимизировать ее, не зная $a_0$.\n-   Единственный способ избавиться от $a_0$ заключается в том, чтобы наложить дополнительное условие $\\sum \\lambda_i - 1 = 0$\n\n## Обычный кригинг\n\nМинимизируем ранее введенную функцию ошибки:\n\n$$\n\\operatorname {Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nДля этого, с учетом дополнительного условия $\\sum \\lambda_i - 1 = 0$ применим метод множителей Лагранжа и построим вспомогательную функцию:\n\n$$\nQ = \\operatorname {Var}[Z^* - Z_0] + 2\\mu \\bigg(\\sum_i \\lambda_i - 1 \\bigg),\n$$\n\nгде $\\mu$ -- неизвестный множитель Лагранжа.\n\n## Обычный кригинг\n\n$$\nQ = \\operatorname {Var}[Z^* - Z_0] + 2\\mu \\bigg(\\sum_i \\lambda_i - 1 \\bigg)$$\n\nДля минимизации функции приравняем нулю ее частные производные:\n\n$$\n\\begin{cases}\\frac{\\partial Q}{\\partial \\lambda_i} = 2 \\sum_j \\lambda_j \\sigma_{ij} - 2 \\sigma_{i0} + 2\\mu = 0,~i = 1,...,N,\\\\\n\\frac{\\partial Q}{\\partial \\mu} = 2\\bigg(\\sum_i \\lambda_i - 1 \\bigg) = 0\n\\end{cases}\n$$\n\n## Обычный кригинг\n\nИмеем $N + 1$ уравнений с $N + 1$ неизвестными:\n\n$$\n\\begin{cases}\\sum_j \\lambda_j \\sigma_{ij} + \\mu = \\sigma_{i0},~i = 1,...,N,\\\\\n\\sum_i \\lambda_i = 1\n\\end{cases}\n$$\n\nЗаменяя ковариацию на вариограмму, получаем **систему уравнений обычного кригинга**:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\begin{cases}\\sum_j \\lambda_j \\gamma_{ij} - \\mu = \\gamma_{i0},\\color{gray}{~i = 1,...,N,}\\\\\n\\sum_i \\lambda_i = 1\n\\end{cases}}}}\n$$\n\n> Это наиболее часто используемый в геостатистике метод оценки\n\n## Дисперсия обычного кригинга\n\nВывод формулы для оценки дисперсии обычного кригинга выполняется аналогично случаю простого кригинга. Умножим $N$ первых уравнений на $\\lambda_i$, просуммируем их по $i$:\n\n$$\n\\sum_j \\lambda_j \\gamma_{ij} - \\mu = \\gamma_{i0}~\\Bigg|\\times \\lambda_i\n$$\n\nУчтя дополнительное условие $\\sum_i \\lambda_i = 1$, получаем выражение для оценки дисперсии (ошибки) обычного кригинга:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\sigma_{OK} = \\operatorname {Var}[Z^* - Z_0] = \\sum_{i}\\lambda_i\\gamma_{i0} - \\mu}}}\n$$\n\n## Универсальный кригинг\n\nВ методе универсального кригинга осуществляется декомпозиция переменной $Z(x)$ в виде следующей суммы:\n\n$$\nZ(x) = m(x) + Y(x)\n$$\n\n-   $m(x)$ — **дрифт** (*drift*), гладкая детерминированная функция, описывающая *систематическую* составляющую пространственной изменчивости явления;\n\n-   $Y(x)$ - **остаток** (*residual*), случайная функция с нулевым математическим ожиданием, описывающая *случайную* составляющую пространственной изменчивости явления;\n\n> Декомпозиция любого явления на дрифт и остаток зависит от масштаба рассмотрения явления.\n\n## Универсальный кригинг\n\nМетод универсального кригинга используется, когда математическое ожидание случайного процесса непостоянно по пространству. Это позволяет интерполировать данные, в которых присутствует тренд.\n\nВ предположении, что м.о. имеет функциональную зависимость от других процессов в точке $x$, вводится следующая модель:\n\n$$\nm(x) = \\sum_{k=0}^{K} a_k f^k(x),\n$$\n\nгде $f^k(x)$ — известные *базисные функции*, а $a_k$ — фиксированные для точки $x$, но неизвестные коэффициенты.\n\n## Универсальный кригинг\n\n$$\nm(x) = \\sum_{k=0}^{K} a_k f^k(x),\n$$\n\n-   Обычно первая базисная функция при $k = 0$ представляет собой константу, равную 1. Это позволяет включить в модель случай постоянного м.о.\n\n-   Если среднее зависит только от местоположения, то оставшиеся функции $f^k(x), k > 0$, как правило, представляют собой одночлены от координат (например, для двумерного случая $f^2(p) = x^2 + y^2$)\n\n-   Коэффициенты $a_k$ могут меняться в зависимости от $x$, но обязательно медленно, чтобы их можно было считать постоянными в окрестности $x$.\n\n## Универсальный кригинг\n\nВ качестве дрифта $m(x)$ можно использовать не только функцию от местоположения, но также значения внешней переменной — **ковариаты**.\n\nНапример, количество осадков можно связать с высотой точки $H(x)$ следующей моделью:\n\n$$\nZ(x) = a_0 + a_1 H(x) + Y(x)\n$$\n\nС статистической точки зрения это линейная регрессия, в которой остатки коррелированы (автокоррелированы).\n\n> В литературе данный метод называют также **регрессионным кригингом** (*regression kriging*), а оценка дрифта — **пространственной регрессией** (*spatial regression*).\n\n## Универсальный кригинг\n\nДля вывода уравнений рассмотрим среднеквадратическую ошибку:\n\n$$\n\\operatorname E[Z^* - Z_0]^2 = \\operatorname {Var}[Z^* - Z_0] + \\big(\\operatorname E[Z^* - Z_0]\\big)^2\n$$\n\nИспользуя введенную модель дрифта $m(x) = \\sum_{k=0}^{K} a^k f^k(x)$ распишем выражение для мат.ожидания приращений:\n\n$$\n\\operatorname E[Z^* - Z_0] = \\operatorname E[Z^*\\big] - \\operatorname E[Z_0] = \\\\\n\\sum_i \\lambda_i \\sum_k a_k f_i^k - \\sum_k a_k f_0^k = \\sum_k a_k \\Bigg(\\sum_i \\lambda_i f_i^k - f_0^k\\Bigg)\n$$\n\n## Универсальный кригинг\n\n$$\n\\operatorname E[Z^* - Z_0] = \\sum_k a_k \\Bigg(\\sum_i \\lambda_i f_i^k - f_0^k\\Bigg)\n$$\n\nЧтобы минимизировать $\\operatorname E[Z^* - Z_0]$ независимо от коэффициентов $a_k$, достаточно в вышеприведенной формуле приравнять нулю выражение в скобках. Отсюда имеем:\n\n$$\\sum_i \\lambda_i f_i^k = f_0^k,~k = 0, 1, ..., K.$$\n\nЭти условия называются **условиями универсальности**. Отсюда идет название метода — *универсальный кригинг*\n\n> Условия универсальности гаранируют, что оценка $Z^*$ является **несмещенной** для *любых* значений $a_k$.\n\n## Универсальный кригинг\n\nМинимизируем ранее введенную функцию ошибки:\n\n$$\n\\operatorname {Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nДля этого, с учетом дополнительного условия $\\sum_i \\lambda_i f_i^k = f_0^k$ применим метод множителей Лагранжа и построим вспомогательную функцию:\n\n$$Q = \\operatorname {Var}[Z^* - Z_0] + 2 \\sum_{k=0}^K \\mu_k \\Bigg[ \\sum_i \\lambda_i f_i^k - f_0^k\\Bigg],$$\n\nгде $\\mu_k,~k = 0, 1, ..., K$ представляют $K + 1$ дополнительных неизвестных, множители Лагранжа.\n\n## Универсальный кригинг\n\n$$\n\\operatorname {Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nДля минимизации функции приравняем нулю ее частные производные:\n\n$$\n\\begin{cases}\\frac{\\partial Q}{\\partial \\lambda_i} = 2 \\sum_j \\lambda_j \\sigma_{ij} -2 \\sigma_{i0} + 2 \\sum_k \\mu_k f_i^k = 0,\\color{gray}{~i = 1,...,N,}\\\\\n\\frac{\\partial Q}{\\partial \\mu} = 2\\bigg[\\sum_i \\lambda_i f_i^k - f_0^k \\bigg] = 0\\color{gray}{,~k = 0, 1,..., K.}\n\\end{cases}\n$$\n\n## Универсальный кригинг\n\nИмеем систему из $N + K + 1$ уравнений с $N + K + 1$ неизвестными:\n\n$$\n\\begin{cases}\\sum_j \\lambda_j \\sigma_{ij} + \\sum_k \\mu_k f_i^k = \\sigma_{i0},~i = 1,...,N,\\\\\n\\sum_i \\lambda_i f_i^k = f_0^k,~k = 0, 1,..., K.\n\\end{cases}\n$$\n\nЗаменяя ковариацию на вариограмму, получаем **систему уравнений универсального кригинга**:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\begin{cases}\\sum_j \\lambda_j \\gamma_{ij} - \\sum_k \\mu_k f_i^k = \\gamma_{i0},\\color{gray}{~i = 1,...,N,}\\\\\n\\sum_i \\lambda_i f_i^k = f_0^k\\color{gray}{,~k = 0, 1,..., K.}\n\\end{cases}}}}\n$$\n\n## Дисперсия универсального кригинга\n\nВывод формулы для оценки дисперсии универсального кригинга выполняется аналогично случаю обычного кригинга. Умножим $N$ первых уравнений на $\\lambda_i$, просуммируем их по $i$:\n\n$$\n\\sum_j \\lambda_j \\gamma_{ij} - \\sum_k \\mu_k f_i^k = \\gamma_{i0} ~ \\Bigg|\\times \\lambda_i\n$$\n\nУчтя дополнительное условие $\\sum_i \\lambda_i f_i^k = f_0^k$, получаем выражение для оценки дисперсии (ошибки) универсального кригинга:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\sigma^2_{UK} = E[Z^* - Z_0]^2 = \\sum_{i}\\lambda_i\\gamma_{i0} - \\sum_k \\mu_k f_0^k}}}\n$$\n\n## Диаграмма рассеяния с лагом\n\n**Lagged scatterplot** — вариант диаграммы рассеяния, на котором показываются значения в точках, расстояние между которыми попадает в заданный интервал\n\n\n::: {.cell layout-align=\"center\" warnin='false'}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n## Вариограммное облако\n\nКвадрат разности значений как функция от расстояния между точками\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-2-1.png){width=3000}\n:::\n:::\n\n\n## Эмпирическая вариограмма\n\nЭмпирическая вариограмма рассчитывается путем разбиения вар. облака на интервалы расстояний — **лаги** — и подсчета среднего значения $\\gamma$ в каждом лаге:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-3-1.png){width=3000}\n:::\n:::\n\n\n## Эмпирическая вариограмма\n\n$$\\hat{\\gamma} = \\frac{1}{2N_h} \\sum_{x_i - x_j \\approx h} \\big[z(x_i) - z(x_j)\\big]^2$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-4-1.png){width=3000}\n:::\n:::\n\n\nРазмер точки означает количество пар значений, которые попали в каждый лаг.\n\n## Эмпирическая вариограмма\n\nПоскольку вариограмма есть *дисперсия разности значений*, ее рост можно оценить также по увеличению размера «ящика» на диаграмме размаха $\\sqrt\\gamma$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-5-1.png){width=3000}\n:::\n:::\n\n\n## Вариокарта\n\n**Вариокарта** (*variogram map, variomap*) представляет вариограмму как функцию приращений координат: $$\\hat{\\gamma} (\\Delta x, \\Delta y) = \\frac{1}{2N_{\\substack{\\Delta x\\\\ \\Delta y}}} \\sum_{\\substack{\\Delta x_{ij} \\approx \\Delta x\\\\ \\Delta y_{ij} \\approx \\Delta y}} \\big[z(p_i) - z(p_j)\\big]^2$$\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\nВариокарта используется для выявления *пространственной анизотропии*. Профиль по линии из центра к краю вариокарты даст эмпирическую вариограмму\n:::\n:::\n\n## Эмпирическая и теоретическая вариограмма\n\n**В уравнениях кригинга нельзя использовать эмпирическую вариограмму.**\n\n-   Это связано с тем, что уравнения ординарного (обычного) кригинга выводятся из предположения о том, что вариограмма является условно положительно определенной функцией.\n\n-   Выполнение этого условия нельзя гарантировать для эмпирической кривой.\n\n-   Необходимо использовать теоретическую функцию — *модель вариограммы* — которая отвечает условной положительной определенности.\n\n-   Модель вариограммы выбирается исходя из формы эмпирической вариограммы.\n\n## Свойства вариограммы\n\n$$\\gamma (\\mathbf{h}) = \\gamma(\\mathbf{x}, \\mathbf{x + h}) = \\operatorname E\\big[Z(x + h)-Z(x)\\big]^2$$\n\nИсходя из определения $\\gamma$, можно вывести ряд обязательных свойств модели:\n\n-   Вариограмма симметрична:\n\n    $$\\gamma(\\mathbf{h}) = \\gamma(-\\mathbf{h})$$\n\n-   Вариограмма связана с дисперсией:\n\n    $$\\gamma(\\infty) = \\operatorname{Var}\\big[ Z(\\mathbf{x}) \\big]$$\n\n-   Вариограмма связана с ковариацией:\n\n    $$\\gamma(\\mathbf{h}) = \\operatorname{Var}\\big[Z(\\mathbf{x}) \\big] - \\operatorname C(\\mathbf{h})$$\n\n## Приближение теоретической модели\n\n**Приближение** (*fitting*) модели вариограммы предполагает:\n\n1.  Выбор теоретической модели\n2.  Подбор параметров модели: эффект самородка (*nugget*), радиус корреляции ( $a$ или *range*) и плато (*sill*).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-7-1.png){width=3000}\n:::\n:::\n\n\n## Сферическая модель\n\n$$\\gamma(h) = \\begin{cases}\n  c_0 + c\\Big[\\frac{3h}{2a} - \\frac{1}{2}\\big(\\frac{h}{a}\\big)^3\\Big], & h \\leq a; \\\\\n  c_0 + c, & h > a.\n\\end{cases}$$\n\n$$\\gamma(a) = \\operatorname{Var}[Z(p)] = c_0 + c$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n<!-- - Данная модель достигает плато в точке $h = a$. -->\n\n## Экспоненциальная модель\n\n$$\\gamma(h) = \\begin{cases}\n  0, & h = 0; \\\\\n  c_0 + (c-c_0)\\Big[1 - \\exp\\big(\\frac{-3h}{a}\\big)\\Big], & h \\neq 0.\n\\end{cases}$$\n\n$$\\gamma(a) = \\operatorname{Var}[Z(p)] = c_0 + c$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n## Экспоненциальная модель\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n-   Данная модель достигает плато асимптотически.\n-   В точке $h = a$ достигается $95\\%$ уровня плато.\n\n## Гауссова модель\n\n$$\\gamma(h) = c_0 + c\\Bigg[1 - \\exp\\bigg(\\frac{-3h^2}{a^2}\\bigg)\\Bigg]$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n## Гауссова модель\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n-   Данная модель достигает плато асимптотически.\n-   В точке $h = a$ достигается $95\\%$ уровня плато.\n-   Отличительной чертой этой модели является ее гладкость: параболическое поведение вблизи нуля и асимптотическое приближение к плато.\n\n## Степенная модель\n\n$$\\gamma(h) = \\begin{cases}\n  0, & h = 0; \\\\\n  c h^\\alpha, & h \\neq 0.\n\\end{cases}$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n## Степенная модель\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n-   Автокорреляция присутствует на всех расстояниях: $a \\rightarrow \\infty$\n-   Предположение о стационарности второго порядка не выполняется\n-   Как правило, это означает наличие тренда в данных\n\n## Эффект самородка (модель наггет)\n\n$$\\gamma(h) = \\begin{cases}\n  0, & h = 0; \\\\\n  c_0, & h \\neq 0.\n\\end{cases}, ~ c_0 = C(0)$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/sam-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n## Эффект самородка (модель наггет)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n-   Наличие у данных вариограммы типа наггет означает отсутствие пространственной корреляции.\n-   Возможные причины:\n    -   Абсолютно случайное распределение\n    -   Мелкомасштабная вариабельность (меньше, чем расстояние между измерениями)\n    -   Ошибки в измерениях\n    -   Ошибки в координатах точек\n\n## Автоматическое приближение\n\nДана вариограмма семейства $\\gamma (h; \\mathbf{b})$, где $\\mathbf{b} = (b_1, ..., b_k)$ — вектор из $k$ параметров модели. Параметры $\\mathbf{b}$ подбираются таким образом, чтобы минимизировать следующий функционал:\n\n$$Q(\\mathbf{b}) = \\sum_{l=1}^{L} w_l \\big[\\hat{\\gamma}(h_l) - \\gamma (h; \\mathbf{b})\\big]^2,$$\n\nгде $\\big\\{\\hat{\\gamma} (h_l): l = 1,...,L\\big\\}$ — значения эмпирической вариограммы для $L$ лагов, вычисленные по $N(h_l)$ векторам.\n\nВеса $w_l$ обычно выбираются исходя из отношения $w_l = N(h_l) / |h_l|$, чтобы придать большее значение коротким расстояниям и лагам с хорошей оценкой.\n\n## Автоматическое приближение\n\n$$Q(\\mathbf{b}) = \\sum_{l=1}^{L} w_l \\big[\\hat{\\gamma}(h_l) - \\gamma (h; \\mathbf{b})\\big]^2$$ Минимизация функционала осуществляется итеративно:\n\n1.  Процесс начинается с некоторого предположения $\\mathbf{b}^{(0)}$\n2.  На шаге $s$ функция $Q$ аппроксимируется в виде квадратичной формы $Q(\\mathbf{b}^{(s)}) \\approx \\sum_{i=1}^k \\sum_{j=1}^k \\delta_{ij} b_i b_j$ путем разложения в ряд Тейлора вокруг точки $\\mathbf{b}^{(s)}$.\n3.  Новая точка минимума $\\mathbf{b}^{(s+1)}$ находится как минимум квадратичной формы (*этот минимум один*).\n\nШаги 2-3 повторяются до тех пор, пока значение $Q$ не станет меньше заданного порога.\n\n## Автоматическое приближение\n\nСравним результат ручного и автоматического приближения вариограммы:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-16-1.png){width=3000}\n:::\n:::\n\n\n## Обычный кригинг\n\nРассмотрим данные по температуре:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-17-1.png){width=3000}\n:::\n:::\n\n\n## Обычный кригинг\n\nВизуализируем найденную вариограмму:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-18-1.png){width=3000}\n:::\n:::\n\n\n## Обычный кригинг\n\nВизуализируем найденную вариокарту:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-19-1.png){width=3000}\n:::\n:::\n\n\n## Обычный кригинг\n\nПроинтерполируем, используя приближенную модель вариограммы:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[using ordinary kriging]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstars object with 2 dimensions and 2 attributes\nattribute(s):\n                 Min.   1st Qu.   Median     Mean  3rd Qu.      Max.\nvar1.pred  -0.4091735  7.707571 18.83325 21.50978 32.07393  67.26636\nvar1.var   30.9929191 45.435980 52.71968 58.67491 65.48474 186.22488\ndimension(s):\n  from  to  offset delta                refsys x/y\nx    1 213  332239  2000 WGS 84 / UTM zone 32N [x]\ny    1  99 5121556 -2000 WGS 84 / UTM zone 32N [y]\n```\n\n\n:::\n:::\n\n\n## Оценка кригинга\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-21-1.png){width=3000}\n:::\n:::\n\n\n## Дисперсия кригинга\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-22-1.png){width=3000}\n:::\n:::\n\n\nДисперсия кригинга высока там, где мало данных\n\n## Среднеквадратическое отклонение кригинга\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-23-1.png){width=3000}\n:::\n:::\n\n\nДля удобства интерпретации можно взять корень из дисперсии\n\n## Кросс-валидация\n\nЗначение переменной $Z(x)$ оценивается в каждой точке $x_i$ по данным в соседних точках $Z(x_j), ~ j \\neq i$ как если бы $Z(x_i)$ было неизвестно.\n\nВ каждой точке вычисляется оценка кригинга $Z_{-i}^*$ и соответствующая дисперсия кригинга $\\sigma_{Ki}^2$. Поскольку значение $Z_i = Z(x_i)$ известно, мы можем вычислить:\n\n-   **Ошибку кригинга** $E_i = Z_{-i}^* - Z_i$\n-   **Стандартизированную ошибку** $e_i = E_i / \\sigma_{Ki}$\n\nЕсли $\\gamma(h)$ — теоретическая вариограмма, то $E_i = Z_{-i}^* - Z_i$ — случайная величина с м.о. = $0$ и дисперсией $\\sigma_{Ki}^2$, а $e_i$ имеет м.о. = $0$ и дисперсию, равную $1$.\n\n## Кросс-валидация\n\n-   **Ошибка кригинга** $E_i = Z_{-i}^* - Z_i$\n-   **Стандартизированная ошибка** $e_i = E_i / \\sigma_{Ki}$\n\nСтандартно анализируются следующие карты и графики:\n\n-   *Карта стандартизированных ошибок* $e_i$. Стационарность ошибок, отсутствие эффекта пропорциональности.\n\n-   *Гистограмма стандартизированных ошибок* $e_i$. Нормальность распределения, отсутствие аномалий.\n\n-   *Диаграмма рассеяния* $(Z_{-i}^*, Z_i)$. Сглаживающий эффект, соответствие оценки и реального значения.\n\n-   *Диаграмма рассеяния* $(Z_{-i}^*, e_i)$. Независимость (ортогональность) оценки и ошибки.\n\n## Кросс-валидация\n\nДля выполнения кросс-валидации воспользуемся функцией `krige.cv`:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   var1.pred var1.var observed     residual      zscore fold       sterr\n1   5.743730 34.84033      6.0   0.25627005  0.04341669    1  0.04341669\n2  11.137129 60.24070     10.0  -1.13712865 -0.14650910    2 -0.14650910\n3   6.929502 47.22732      7.0   0.07049833  0.01025846    3  0.01025846\n4  23.252858 48.06354      1.0 -22.25285758 -3.20979954    4 -3.20979954\n5  15.655167 56.76258      1.0 -14.65516724 -1.94517957    5 -1.94517957\n6  11.794241 44.03055      1.0 -10.79424095 -1.62672846    6 -1.62672846\n7  11.325378 62.65261      0.1 -11.22537769 -1.41818009    7 -1.41818009\n8  28.421330 75.24988      0.2 -28.22133030 -3.25330355    8 -3.25330355\n9   2.340115 58.30350      1.0  -1.34011550 -0.17550719    9 -0.17550719\n10  3.489972 62.96551      0.2  -3.28997242 -0.41461106   10 -0.41461106\n```\n\n\n:::\n:::\n\n\n## Кросс-валидация\n\nCтандартизированные ошибки в стационарном случае должны быть распределены нормально:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-25-1.png){width=3000}\n:::\n:::\n\n\n## Кросс-валидация\n\nОшибки должны быть независимы от значений:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-26-1.png){width=3000}\n:::\n:::\n\n\n## Кросс-валидация\n\nОблако рассеяния оценки относительно истинных значений должно быть компактным:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-27-1.png){width=3000}\n:::\n:::\n\n\n## Кросс-валидация\n\nПространственная картина стандартизированных ошибок должна быть гомогенной:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_Geostatistics_files/figure-revealjs/unnamed-chunk-28-1.png){width=2100}\n:::\n:::\n\n\n## Библиография\n",
    "supporting": [
      "02_Geostatistics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}