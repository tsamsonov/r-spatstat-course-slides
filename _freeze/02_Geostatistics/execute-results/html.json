{
  "hash": "2c2679951da32fe9f6fe36b6a4c5ea31",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Геостатистика\"\nsubtitle: \"Пространственная статистика\"\ndate: today\ndate-format: long\nauthor: \"Самсонов Тимофей Евгеньевич\"\nexecute:\n  echo: false\n  freeze: true\nengine: knitr\nformat:\n  revealjs: \n    theme: [default, custom.scss]\n    margin: 0.2\n    width: 1280\n    height: 720\n    slide-number: true\n    footer: \"Самсонов Т. Е. Пространственная статистика: курс лекций\"\n    header-includes: <link rel=\"stylesheet\" media=\"screen\" href=\"https://fontlibrary.org//face/pt-sans\" type=\"text/css\"/>\nbibliography: references.yaml\nmainfont: PT Sans\n---\n\n\n## Базовые компоненты\n\n1.  Пространственные локации (точки) $$\\{p_1, p_2, ..., p_n\\}$$\n2.  Данные в этих локациях $$\\{Z(p_1), Z(p_2), ..., Z(p_n)\\}$$ \n\n> Обе компоненты в общем случае являются случайными\n\n## Случайная величина\n\n> **Случайной величиной** $Z(w)$ называется функция, которая в результате случайного события $w$ принимает некоторое вещественнозначное значение.\n\n::: columns\n::: {.column width=\"50%\"}\nНапример, при анализе температуры водоема в отдельно взятой точке в толще воды случайной величиной (функцией) является собственно температура, а событием — та совокупность физико-химических условий, которая сложилась в данной точке в момент измерений.\n:::\n\n::: {.column width=\"50%\"}\n*Элемент случайности вносится именно событием*, которое в природе может быть чрезвычайно сложной и трудно предсказуемой комбинацией факторов, в то время как *случайная величина связана с событием функциональной зависимостью*.\n:::\n:::\n\n## Пространственная модель\n\n$p \\in \\mathbb{R}^k$ — точка в $k$-мерном Евклидовом пространстве\n\n$Z(p)$ — **случайная величина** в точке $p$\n\nЕсли $p$ меняется над индексным множеством $D \\subset \\mathbb{R}^k$, то формируется **случайный процесс**:\n\n$$\n\\{Z(p) | p \\in D\\}\n$$ \n\nРезультат наблюдения случайного процесса в точках $D$ является **реализацией** случайного процесса:\n\n$$\n\\{z(p) | p \\in D\\}\n$$ \n\n> В общем случае $D$ и $Z$ случайны и независимы\n\n## Случайный процесс (функция)\n\n> **Случайный процесс** — это семейство случайных величин, индексированных некоторым параметром $t$\n\n-   Наиболее часто анализируются одномерные случайные процессы, в которых $t$ — это время\n\n-   Пример случайного процесса — температура не в один момент времени, а в течение некоторого промежутка времени\n\n-   Пространственная статистика изучает случайные процессы, в которых $t$ — это координата точки (обычно на плоскости)\n\n## Случайный процесс (функция)\n\n-   В каждой точке $p_i$ существует некоторая *случайная величина* $Z(p_i)$ — **сечение случайного процесса**\n\n-   При изменении точки $p_i$ наблюдаемое значение случайного процесса меняется случайным образом, поскольку определяется оно не только местоположением, но и заранее неизвестным случайным событием\n\n-   Большинство природных явлений показывают зависимость наблюдаемых значений от их взаимного местоположения, которая проявляется в наличии корреляции значений $Z(\\mathbf{p})$ и $Z(\\mathbf{p} + \\mathbf{h})$, где $\\mathbf{h}$ — вектор смещения между точками\n\n-   Тем короче $h$, тем, как правило, сильнее выражена корреляция значений\n\n## Автокорреляция\n\n::: columns\n::: {.column width=\"40%\"}\n![](images/Student.jpg){width=\"100%\"}\n:::\n\n::: {.column width=\"60%\"}\nСтьюдент в письме Карлу Пирсону (1900):\n\n> *...В целом, корреляция ослабевает, если охват по времени или пространству увеличивается. Меня не покидает мысль, что было бы великим достижением установить закон, согласно которому корреляция будет ослабевать с увеличением охвата*\n:::\n:::\n\n## Автокорреляция\n\nБританский статистик и биолог **Рональд Эйлмер Фишер** изучал пространственное распределение характеристик растений на опытных площадках, будучи сотрудником Ротамстедской агрохимической станции в 1920-1930-е гг.\n\n::: columns\n::: {.column width=\"40%\"}\n![Sir Ronald Aylmer Fisher (1876-1937)](images/Fisher.png){width=\"70%\"}\n:::\n\n::: {.column width=\"60%\"}\nСтьюдент в письме Карлу Пирсону (1900):\n\n> ...всестороннее подтверждение получил тот факт, что близко расположенные участки более схожи, чем удаленные, судя по данным об урожайности... следовательно, наиболее верным будет делать \\[выборочные\\] блоки максимально компактными\\_\n>\n> <footer>--- «The Design of Experiments» (1935)</footer>\n:::\n:::\n\n## Пространственная статистика\n\n1.  **Геостатистика** *(geostatistics)*\n    -   $D$ — фиксированное подмножество в $\\mathbb{R}^k$\n    -   $Z(p)$ — случайный вектор в каждой точке $p$\n    -   Исследуется пространственное распределение\n\n2.  **Сеточные данные** *(lattice data)*\n\n    -   $D$ — фиксированное счетное подмножество точек $\\mathbb{R}^k$\n    -   $Z(p)$ — случайный вектор в каждой точке $p$\n    -   Исследуется пространственная зависимость и гетерогенность\n\n3.  **Конфигурации точек** *(point patterns)*\n\n    -   $D$ — счетное подмножество точек $\\mathbb{R}^k$ *(точечный процесс)*\n    -   $Z(p)$ — константа или счетное множество *(маркированный точечный процесс)*\n    -   Исследуется пространственное размещение\n\n## Математическое ожидание\n\n**Математическое ожидание** — наиболее вероятная реализация случайного процесса:\n\n$$\n\\operatorname E[Z(p)]=m(p)\n$$\n\n-   Пусть дан географический регион, в котором производятся наблюдения температуры в течение месяца.\n\n-   В каждый момент времени мы имеем непрерывное поле температуры — реализацию случайного процесса и относящиеся к ней данные наблюдений на метеостанциях\n\n-   Осреднив данные за период наблюдений восстановим выборочное среднее поле распределения температур — оценку математического ожидания случайного процесса\n\n## Дисперсия\n\n**Дисперсия** — мера разброса реализаций случайного процесса относительно его математического ожидания:\n\n$$\n\\operatorname {Var}[Z(p)]= \\operatorname E[Z^2(p)]-m^2(p)\n$$\n\n> Аналогично математическому ожиданию, дисперсия двумерного с.п. представляет собой *поле распределения*, значение которого в каждой точке равно дисперсии локального сечения с.п.\n\n## Ковариация\n\n**Ковариация** — мера линейной зависимости сечений случайного процесса в двух точках $p_1$ и $p_2$:\n\n$$\n\\operatorname {Cov}(p_1,p_2) = \\operatorname {Cov}[Z(p_1), Z(p_2)] = \\\\\\operatorname E[Z(p_1)Z(p_2)]-m(p_1)m(p_2)\n$$\n\n> Недостатком ковариации является необходимость знания математического ожидания с.п. Это условие выполняется далеко не всегда, что связано с тем что как правило приходится иметь дело только с одной реализацией с.п\n\n## Свойства моментов случайных процессов\n\n-   Моменты пространственных случайных процессов являются **функциями**, а не *константами*, в отличие от моментов случайных величин.\n\n-   Давать оценку пространственной структуре явления на основе вычисленных моментов с.п. можно только при условии, что он удовлетворяет свойствам **стационарности** и **эргодичности.**\n\n## Гипотеза стационарности\n\n-   Функции корреляции между данными зависят только от взаимного расположения точек измерений, а не от их конкретного местоположения в пространстве.\n\n-   В этом случае пространственная корреляция определяется вектором $\\mathbf{h}$ между точками.\n\n-   Для изотропного случая, когда корреляция не зависит от направления, а только от расстояния вектор $\\mathbf{h}$ переходит в скаляр — расстояние $d$.\n\n## Стационарность\n\n**Стационарность** *в строгом смысле* означает что функция распределения множества случайных величин для любой комбинации точек ${x_1, x_2,...,x_k}$ и любого $k < \\infty$ остается неизменной при смещении этой комбинации на произвольный вектор $h$:\n\n$$\n\\operatorname P\\{Z(x_1)<z_1,...,Z(x_k)<z_k\\} = \\\\ \\operatorname P\\{Z(x_1 + h)<z_1,...,Z(x_k + h)<z_k\\}\n$$\n\n## Стационарность\n\n-   Стационарность по другому называют **однородностью в пространстве**, подразумевая что явление ведет себя одинаковым образом в любой точке пространства, как бы повторяет само себя.\n\n-   Если с.ф. стационарна, все ее моменты будут инвариантны относительно сдвигов (то есть будут постоянны), а это означает что для их оценки можно использовать ограниченную в пространстве область.\n\n-   В реальности же подобного рода «идеальное» поведение встречается крайне редко, поэтому используют более слабое предположение о стационарности второго порядка.\n\n## Стационарность второго порядка\n\nСлучайная функция имеет имеет **стационарность второго порядка**, если для любых точек $x$ и $x+h$ в $R^k$\n\n$$\n\\begin{cases}\n  \\operatorname E[Z(x)] = m \\\\\n  \\operatorname E[(Z(x)-m)(Z(x+h)-m)] = \\operatorname C(h)\n\\end{cases}\n$$\n\nМатематическое ожидание с.ф. постоянно, а ковариация зависит только от вектора $h$ между точками и не зависит от их абсолютного положения.\n\nЕсли ковариация также не зависит от направления, а только от расстояния между точками, то $h$ вырождается в скаляр, а такая случайная функция является *изотропной стационарной*.\n\n## Эргодичность\n\nСтационарная случайная функция $Z(x,w)$ называется **эргодической**, если ее среднее по области $V \\subset R^k$ сходится к математическому ожиданию $m(w)$ при стремлении $V$ к бесконечности:\n\n$$\n\\lim_{V \\rightarrow \\infty} \\frac{1}{|V|}\\int_{V} Z(x,w)dx = m(w)\n$$\n\n$|V|$ обозначает *меру* области $V$ (площадь, объем). Предполагается что сама область $V$ растет во всех направлениях, и предел ее роста не зависит от ее формы.\n\n> Cреднее по всем возможным реализациям равно среднему отдельной безграничной в пространстве реализации.\n\n## Эргодичность\n\n-   Дан кувшин с песком, в котором необходимо определить долю объема, занятую содержимым.\n\n-   Зафиксируем некоторую точку $x$ в системе отсчета, привязанной к кувшину, и будем его встряхивать бесконечное число раз, каждый раз фиксируя, оказалась ли точка $x$ внутри песчинки (записываем 1) или же попала в свободное между ними пространство (записываем 0)\n\n-   Из серии подобных экспериментов мы сможем оценить среднее значение индикаторной функции $\\operatorname I(x,w)$, которое равно вероятности попадания зерна в точку $x$, и которое не зависит от $x$.\n\n-   Эта вероятность и будет равна доли объема кувшина, занятой песком.\n\n## Эргодичность\n\n-   Аналогичный результат можно получить, если теперь зафиксировать кувшин, а точку $x$ выбирать каждый раз случайным образом.\n\n-   В первом случае берется среднее по реализациям, а во втором — среднее по пространству.\n\n> В реальных экспериментах приходится иметь дело со вторым случаем.\n\n## Простой кригинг\n\nДля оценки в точке $z_0 = z(p_0)$ по $N$ измерениям $z_1, ..., z_N$ ищутся коэффициенты следующего выражения:\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0\n$$\n\nКонстанта $\\lambda_0$ и веса $\\lambda_i$ подобираются таким образом, что минимизируется среднеквадратическая ошибка:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big],\n$$ то есть математическое ожидание квадрата отклонения оценки от реального значения в точке $p_0$.\n\n## Простой кригинг\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0\n$$\n\nИспользуя соотношение $\\operatorname {Var}[X] = \\operatorname E[X^2] - (\\operatorname E[X])^2$, можно выразить среднюю квадратическую ошибку как:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big] = \\operatorname{Var}[Z^* - Z_0] + (\\operatorname E[Z^* - Z_0])^2\n$$\n\nПоскольку дисперсия нечувствительна к сдвигам, изменение константы $\\lambda_0$ влияет только на компоненту $\\operatorname E[Z^* - Z_0]$. Приравняем ее нулю:\n\n$$\n\\operatorname E[Z^* - Z_0] = \\operatorname E\\Big[\\sum_{i} \\lambda_i Z_i + \\lambda_0 - Z_0\\Big] = 0\n$$\n\n## Простой кригинг\n\n$$\n\\operatorname E[Z^* - Z_0] = \\operatorname E\\Big[\\sum_{i} \\lambda_i Z_i + \\lambda_0 - Z_0\\Big] = 0\n$$\n\nПоскольку $\\lambda_0$ явялется константой, то по свойству мат. ожидания ее можно вынести за скобки:\n\n$$\n\\lambda_0 = -\\operatorname E\\Big[\\sum_{i} \\lambda_i Z_i - Z_0\\Big] = m_0 - \\sum_i \\lambda_i m_i,\n$$\n\nгде $m_i$ — известные значения мат. ожиданий случайной функции в каждой точке исходных данных, $m_0$ — известное мат. ожидание в интерполируемой точке.\n\n## Простой кригинг\n\nИмея:\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + \\lambda_0,\\\\\n\\lambda_0 = m_0 - \\sum_i \\lambda_i m_i,\n$$\n\nПолучаем:\n\n$$\nZ^* = \\sum_{i} \\lambda_i Z_i + m_0 - \\sum_i \\lambda_i m_i = \\\\\nm_0 + \\sum_{i} \\lambda_i (Z_i - m_i)\n$$\n\n## Простой кригинг\n\n$$\nZ^* = m_0 + \\sum_{i} \\lambda_i (Z_i - m_i)\n$$\n\nПоскольку константа $m_0$ известна заранее, задачу оценки можно выполнить для переменной $Y(p) = Z(p) - m(p)$, используя линейную оценку\n\n$$\nY^* = \\sum_{i} \\lambda_i Y_i,\n$$\n\nи прибавляя к полученному результату $m_0$.\n\nОсновной вопрос заключается в нахождении коэффициентов $\\lambda_i$.\n\n## Простой кригинг\n\nПоскольку мы показали, что компонента $\\operatorname E[Z^* - Z_0]$ может быть приравнена нулю, среднеквадратическая ошибка равна дисперсии:\n\n$$\n\\operatorname E\\big[(Z^* - Z_0)^2\\big] = \\operatorname{Var}[Z^* - Z_0]\n$$\n\nИспользуя свойства:\n\n-   $\\operatorname{Var}[X + Y] = \\operatorname{Var}[X] + \\operatorname{Var}[Y] + 2 \\operatorname{Cov}[X, Y]$,\\\n-   $\\operatorname{Var}[-X] = \\operatorname{Var}[X]$,\n-   $\\operatorname{Cov}[X, -Y] = -\\operatorname{Cov}[X, Y]$, получаем:\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname{Cov}[Z^*, Z_0]\n$$\n\n## Простой кригинг\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname {Cov}[Z^*, Z_0]\n$$\n\nРаспишем компоненты этого выражения в терминах ковариации.\n\nПусть $X_1,\\ldots, X_n$ случайные величины, а $Y_1 = \\sum\\limits_{i=1}^n a_i X_i,\\; Y_2 = \\sum\\limits_{j=1}^m b_j X_j$ — их две произвольные линейные комбинации. Тогда:\n\n$$\n\\operatorname {Cov}[Y_1,Y_2] = \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^m a_i b_j \\operatorname {Cov}[X_i,X_j]$$.\n\n## Простой кригинг\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname{Cov}[Z^*, Z_0]\n$$\n\nРаспишем компоненты этого выражения в терминах ковариации.\n\n$\\operatorname{Var}[Z^*] = \\operatorname{Cov}[Z^*, Z^*] = \\operatorname{Cov}\\Big[\\sum_{i} \\lambda_i Z_i, \\sum_{j} \\lambda_j Z_j\\Big] = \\\\ \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\operatorname{Cov}[Z_i, Z_j] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}$\n\n$\\operatorname{Var}[Z_0] = \\operatorname{Cov}[Z_0, Z_0] = \\sigma_{00}$\n\n$\\operatorname{Cov}[Z^*, Z_0] = \\operatorname{Cov}\\Big[\\sum_{i} \\lambda_i Z_i, Z_0\\Big] =\\\\ \\sum_{i} \\lambda_i \\operatorname{Cov}[Z_i, Z_0] = \\sum_{i} \\lambda_i \\sigma_{i0}$\n\n## Простой кригинг\n\nТаким образом, выражение для ошибки\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\operatorname{Var}[Z^*] + \\operatorname{Var}[Z_0] - 2 \\operatorname{Cov}[Z^*, Z_0]\n$$\n\nТрансформируется в\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nДля нахождения минимума этой квадратичной функции необходимо приравнять нулю ее производные по основной переменной $\\lambda$. Выберем в качестве «жертвы» коэффициенты с индексом $i$:\n\n$$\n\\frac{\\partial}{\\partial \\lambda_i} \\operatorname E\\big[(Z^* - Z_0)^2\\big] = 2 \\sum_{j} \\lambda_j \\sigma_{ij} - 2 \\sigma_{i0} = 0\n$$\n\n## Простой кригинг\n\n$$\n\\frac{\\partial}{\\partial \\lambda_i} \\operatorname E\\big[(Z^* - Z_0)^2\\big] = 2 \\sum_{j} \\lambda_j \\sigma_{ij} - 2 \\sigma_{i0} = 0\n$$\n\nТаким образом, система уравнений **простого кригинга** для точки $Z_0$ имеет вид:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}\\color{gray}{,~i = 1,...,N}}}}\n$$\n\n> Уравнения простого кригинга носят чисто теоретический характер. На практике используется метод обычного кригинга, в котором знание среднего значения случайной функции не требуется.\n\n## Дисперсия простого кригинга\n\nСуществует возможность оценить в каждой точке не только величину показателя, но также дисперсию оценки (в случае постоянного мат. ожидания — среднеквадратическую ошибку).\n\nДля этого необходимо коэффициенты $\\lambda_i$, полученные из системы уравнения простого кригинга\n\n$$\n\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}\n$$\n\nподставить в выражение среднеквадратической ошибки\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\n## Дисперсия простого кригинга\n\nУмножим обе части каждого уравнения простого кригинга на $\\lambda_i$ и просуммируем все уравнения по $i$:\n\n$$\n\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}~\\Bigg|\\times \\lambda_i\\\\\n\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} = \\sum_{i}\\lambda_i\\sigma_{i0}\n$$\n\nЗаметим, что левая часть уравнения присутствует в выражении среднеквадратической ошибки:\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\color{red}{\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\n## Дисперсия простого кригинга\n\nЗаменим $\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}$ на $\\sum_{i}\\lambda_i\\sigma_{i0}$ в выражении для ско:\n\n$$\n\\operatorname{Var}[Z^* - Z_0] = \\color{red}{\\sum_{i}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00} =\\\\\n\\sum_{i}\\lambda_i\\sigma_{i0} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\n$$\n\nОтсюда получаем выражение для дисперсии (ошибки) простого кригинга:\n\n$$\n\\color{red}{\\boxed{\\color{blue}{\\sigma_{SK} = \\operatorname{Var}[Z^* - Z_0] = \\sigma_{00} - \\sum_{i}\\lambda_i\\sigma_{i0}}}}\n$$\n\n## Библиография\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}